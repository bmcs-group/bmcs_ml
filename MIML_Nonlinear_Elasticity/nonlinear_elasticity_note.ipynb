{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of a Nonlinear Elastic Law by Black box ANN\n",
    "\n",
    "- **Tensor Definitions**:\n",
    "  - Green-Lagrange strain tensor: $\\mathbf{E} \\in \\operatorname{Sym}_3(\\mathbb{R})$\n",
    "  - Second Piola-Kirchhoff stress tensor: $\\mathbf{S} \\in \\operatorname{Sym}_3(\\mathbb{R})$\n",
    "\n",
    "- **Constitutive Mapping**:\n",
    "  - Constitutive mapping between vectorized tensors: $\\widehat{\\mathbf{S}} = \\mathbf{G}(\\widehat{\\mathbf{E}})$\n",
    "  - $\\mathbf{G}: \\mathbb{R}^6 \\rightarrow \\mathbb{R}^6$\n",
    "  - Voigt notation for strain tensor: \n",
    "    $$\\widehat{\\mathbf{E}} = [E_{11}, E_{22}, E_{33}, 2E_{23}, 2E_{13}, 2E_{12}]^T$$\n",
    "  - Voigt notation for stress tensor: \n",
    "    $$\\widehat{\\mathbf{S}} = [S_{11}, S_{22}, S_{33}, S_{23}, S_{13}, S_{12}]^T$$\n",
    "\n",
    "- **Algorithm (Forward Pass of ANN)**:\n",
    "  - **Input**: $x^{(0)}$\n",
    "  - **Output**: $\\boldsymbol{y}$\n",
    "  - **Loop for $l = 1, \\ldots, N_l - 1$**:\n",
    "    - $$\\boldsymbol{x}^{(l)} = \\phi\\left(\\mathcal{W}^{(l)} \\boldsymbol{x}^{(l-1)} + \\mathbf{b}^{(l)}\\right)$$\n",
    "  - **End Loop**\n",
    "  - **Final Output**:\n",
    "    - $$\\boldsymbol{y} = \\mathcal{W}^{(N_l)} \\boldsymbol{x}^{(N_l-1)} + \\mathbf{b}^{(N_l)}$$\n",
    "\n",
    "- **Training Data**:\n",
    "  - Set of $N_m$ strain-stress data pairs:\n",
    "    $$\\left\\{\\left(\\widehat{\\mathbf{E}}^{(m)}, \\widehat{\\mathbf{S}}^{(m)}\\right)\\right\\}_{m=1}^{N_m}$$\n",
    "\n",
    "- **Loss Function (Squared Error)**:\n",
    "  - To determine the ANN parameters:\n",
    "    $$\\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left(\\left(\\mathcal{N}_\\beta\\left(\\widehat{\\mathbf{E}}^{(m)}\\right)\\right)_k - \\widehat{S}_k^{(m)}\\right)^2$$\n",
    "  - Where:\n",
    "    - $\\left(\\right)_k$: $k$-th entry of vector $\\left(\\right)$\n",
    "    - $d$: Dimensionality of vectorized stress tensor (generally $d=6$ for a solid)\n",
    "\n",
    "- **Scaled Loss Function**:\n",
    "  - To address stress components with varying magnitudes:\n",
    "    $$\\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left(\\frac{\\mathcal{N}_\\beta\\left(\\left(\\widehat{\\mathbf{E}}^{(m)}\\right)\\right)_k - \\widehat{S}_k^{(m)}}{\\sigma_k}\\right)^2$$\n",
    "  - Where $\\sigma_k$ denotes the component-wise standard deviation of the training stress data.\n",
    "  - the loss function is minimized using a stochastic gradient descent algorithm\n",
    "  - gradients with respect to each of the ANN’s parameters are obtained exactly through automatic  differentiation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Objective of Approximation with ANN**:\n",
    "  - Approximate $\\mathbf{G}$ with ANN as: $\\mathbf{G} \\approx \\mathcal{N}_\\theta$\n",
    "  - ANN parameters: \n",
    "    $$\\theta=\\bigcup_{l=1}^{N_l}(\\mathcal{W}^{(l)}, \\mathbf{b}^{(l)})$$\n",
    "    - $\\mathbf{W}^{(l)}$: Tensor-valued parameter\n",
    "    - $\\mathbf{b}^{(l)}$: Vector-valued parameter\n",
    "\n",
    "- **Activation Function**:\n",
    "  - Nonlinear activation function used: $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanics-Based Model Constraints\n",
    "\n",
    "- **General Challenges of ANN Models for Nonlinear Elasticity**:\n",
    "  - When the size of an defined ANN is allowed to grow, it can fit well, in the least squares sense, to data from materials governed by complicated nonlinear elastic laws.\n",
    "  - However, as a **phenomenological model**, it can violate fundamental principles in mechanics, making it less suitable for numerical simulations. This can be due to:\n",
    "    - Imperfect training\n",
    "    - Noisy training data\n",
    "    - Overfitting\n",
    "  - Lack of interpretability of ANNs makes it difficult to evaluate the physical soundness of the model parameters after training.\n",
    "\n",
    "- **Importance of Mechanics-Based Constraints**:\n",
    "  - **Objective**: To enforce mechanics-based constraints in the construction of a data-driven constitutive law to ensure physical validity.\n",
    "  - **Advantages**:\n",
    "    - Embedding **a priori knowledge** of mechanics in a data-driven model helps to:\n",
    "      - Favor learning the structure of a constitutive relation over overfitting.\n",
    "      - Reduce the model's sensitivity to noisy data.\n",
    "      - Promote robustness to inputs outside the training domain.\n",
    "    - Mechanics-based constraints act as a form of **regularization**.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1. Dynamic Stability\n",
    "\n",
    "- **Definition of Dynamic Stability**:\n",
    "  - it is defined as the **ability of a system to always maintain finite kinetic energy when finite work is performed on it**.\n",
    "    $$\\mathbf{S}(\\mathbf{E}) = \\frac{\\partial W}{\\partial \\mathbf{E}}(\\mathbf{E})$$\n",
    "    - Where $W: \\operatorname{Sym}_3(\\mathbb{R}) \\rightarrow \\mathbb{R}$ represents the **strain energy density** of the body.\n",
    "\n",
    "- **Challenges of Standard ANN Approaches**:\n",
    "    $$\\widehat{\\mathbf{S}} = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}})$$\n",
    "    cannot be expected to **necessarily satisfy** the Dynamic Stability.\n",
    "  - Even with **noise-free data** and **zero loss convergence**, the **interpolation** and **extrapolation** by a standard ANN inside and outside the training domain are not guaranteed to be **conservative**.\n",
    "\n",
    "\n",
    "- **Proposed Approach for Guaranteeing Dynamic Stability**:\n",
    "  - To ensure Dynamic Stability for **arbitrary strain inputs**, it is proposed to represent the constitutive law using an ANN that learns the **strain energy density function**:\n",
    "    $$W = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}})$$\n",
    "  \n",
    "- **Novel Training Approach**:\n",
    "  - The ANN parameters $\\theta$ are determined by minimizing:\n",
    "    $$\\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left( \\frac{\\frac{\\partial \\mathcal{N}_\\beta}{\\partial \\hat{\\mathrm{E}}_k}\\left(\\widehat{\\mathbf{E}}^{(m)}\\right) - \\widehat{S}_k^{(m)}}{\\sigma_k} \\right)^2$$\n",
    "  \n",
    "- **Training Process**:\n",
    "  - The **weights** of the ANN are trained so that the **partial derivatives** of the network with respect to the input match the training stress data.\n",
    "  - This promotes the learning of a **strain energy density function** (up to an irrelevant additive constant).\n",
    "  \n",
    "- **Obtaining Stresses**:\n",
    "  - After training, the **stresses** can be obtained by differentiating the ANN with respect to the strains:\n",
    "    $$\\widehat{\\mathbf{S}} = \\frac{\\partial \\mathcal{N}_\\theta}{\\partial \\widehat{\\mathbf{E}}}(\\widehat{\\mathbf{E}})$$\n",
    "  \n",
    "- **Resulting Properties**:\n",
    "  - The resulting strain-stress mapping is **unconditionally Dynamicly Stable** by construction.\n",
    "  - This holds **regardless** of the strain input or the smallest value attained by the training loss.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Leveraging Reverse Mode Automatic Differentiation**:\n",
    "  - **Reverse mode automatic differentiation** can be used to **exactly differentiate** a trained ANN and achieve **computational efficiency** during the online computation of stresses.\n",
    "  - It is particularly efficient for obtaining the **Jacobian** of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, where $m \\gg n$.\n",
    "    - In the context of this article, where the ANN learns the **strain energy density**, $n = 1$ and $m = 6$.\n",
    "  - The **gradient** of the ANN with respect to all its inputs can be obtained at roughly the **same computational cost** as for a single function evaluation.\n",
    "  - In contrast, using **finite differencing** to compute the gradient would require **at least $m + 1$ function evaluations** and would be affected by **numerical errors**.\n",
    "\n",
    "- **Eager Execution vs. Graph Execution**:\n",
    "  - There is a notable difference in the **online computational cost** between **eager execution** and **graph execution** of the ANN model.\n",
    "  - **Eager Execution**:\n",
    "    - Interprets the code and executes it in **real-time**.\n",
    "    - The evaluation of the constitutive law involves evaluating $W$, constructing the **backwards graph**, and propagating through the backwards graph.\n",
    "    - This approach introduces **unnecessary computations** and **software-related computational overhead**.\n",
    "  - **Graph Execution**:\n",
    "    - Interprets the code as a **graph**.\n",
    "    - The backwards graph is constructed and **compiled offline**.\n",
    "    - Online evaluation of the constitutive law requires only **propagation through a single graph** that directly relates the strains to the stresses.\n",
    "    - This approach is **computationally more economical**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2: Objectivity\n",
    "\n",
    "- **Concept of Objectivity**:\n",
    "  - **Objectivity** is the concept of **material frame indifference**—the position or orientation of an observer should not affect any quantity of interest.\n",
    "\n",
    "- Satisfeis by writing energy density function as a function of Green–Lagrange strain\n",
    "    $$W = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3: Material Stability\n",
    "\n",
    "- **Concept of Material Stability**:\n",
    "  - **Material stability** ensures that **small loads** do not lead to **arbitrary deformations**.\n",
    "- We need to enforce convexity condition on  $W = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}})$ with respect to its input. We shoud use input-convex neural networks (ICNNs).\n",
    "  1. All weights, except those connected directly to the input, are non-negative.\n",
    "  2. Activation functions are convex and non-decreasing.\n",
    "\n",
    "- To ensure positive weights compatible with gradient-based optimization, weights are expressed as:\n",
    "  $$\n",
    "  \\mathcal{W}_{ij}^{(l)} = \\operatorname{Softplus}(\\mathcal{Q}_{ij}^{(l)}; \\alpha) = \\frac{1}{\\alpha^2} \\log\\left(1 + e^{\\alpha^2 Q_{ij}^{(l)}}\\right)\n",
    "  $$\n",
    "\n",
    "  where $\\mathcal{Q}_{ij}^{(l)}$ and $\\alpha$ are trainable parameters, always resulting in non-negative weights.\n",
    "\n",
    "- Passthrough layers with unconstrained weights $\\widetilde{W}_{ij}^{(l)}$ can also be included to improve predictive power without sacrificing convexity.\n",
    "\n",
    "- Any convex function $f$ of the input vector $\\boldsymbol{x}$ can be added to the ANN's output $y$.\n",
    "    - In this work, the following convex function is used:\n",
    "    $$\n",
    "    f(\\boldsymbol{x}) = \\boldsymbol{x}^T \\mathbf{A}^T \\mathbf{A} \\boldsymbol{x}\n",
    "    $$\n",
    "    - Here, $\\mathbf{A}$ is a matrix-valued trainable parameter of the ANN.\n",
    "\n",
    "- Parameters of the Network**:\n",
    "    - The complete set of parameters for the network is given by:\n",
    "    $$\n",
    "    \\theta = \\left( \\mathbf{A}, \\alpha, \\bigcup_{l=1}^{N_l} \\left( \\mathcal{Q}^{(l)}, \\widetilde{\\boldsymbol{w}}^{(l)}, \\mathbf{b}^{(l)} \\right) \\right)\n",
    "    $$\n",
    "    - $\\mathcal{Q}$ and $\\widetilde{\\mathcal{W}}$ are matrices that collect the parameters $\\mathcal{Q}_{ij}$ and $\\widetilde{\\boldsymbol{W}}_{ij}$, respectively.\n",
    "\n",
    "- ICNN Algorithm\n",
    "\n",
    "  - **Input**: $\\boldsymbol{x}^{(0)}$\n",
    "  - **Output**: $y$\n",
    "\n",
    "  1. $\\boldsymbol{x}^{(1)} = \\phi\\left(\\widetilde{\\mathcal{W}}^{(1)} \\boldsymbol{x}^{(0)} + \\mathbf{b}^{(1)}\\right)$\n",
    "  2. **For** $l = 2, \\ldots, N_l - 1$:\n",
    "    $$\n",
    "    \\boldsymbol{x}^{(l)} = \\phi\\left(\\operatorname{Softplus}\\left(\\mathcal{Q}^{(l)}; \\alpha\\right) \\boldsymbol{x}^{(l-1)} + \\mathbf{b}^{(l)} + \\widetilde{\\mathcal{W}}^{(l)} \\boldsymbol{x}^{(0)}\\right)\n",
    "    $$\n",
    "  3. **End For**\n",
    "  4. $y = \\operatorname{Softplus}\\left(\\mathcal{Q}^{(N_l)}; \\alpha\\right) \\boldsymbol{x}^{(N_l-1)} + f(\\boldsymbol{x}^{(0)})$\n",
    "\n",
    "- Activation Function Requirement\n",
    "\n",
    "  To differentiate the ANN (for both stress computation and tangent modulus), the activation function must be:\n",
    "  - At least twice differentiable\n",
    "  - Convex and non-decreasing\n",
    "  - Have non-vanishing second derivatives\n",
    "\n",
    "  Popular activation functions such as tanh, ReLU, ELU, and Softplus do not meet these requirements. Therefore, a new activation function, **SoftplusSquared**, is proposed:\n",
    "\n",
    "  $$\n",
    "  \\phi(z) = \\operatorname{SoftplusSquared}(z; \\beta) = \\frac{1}{2\\beta^4} \\log\\left(1 + e^{\\beta^2 z}\\right)^2\n",
    "  $$\n",
    "\n",
    "  where $\\beta$ is a trainable parameter that controls the curvature of the function at the origin.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4: Consistency\n",
    "\n",
    "1. **Definition of Consistent Material Law**:\n",
    "    - A consistent material law ensures that a numerical computation, such as a finite element (FE) analysis, preserves the rigid body modes of a structure.\n",
    "    - This means mapping a state of zero strain onto a state of zero stress, ensuring $\\mathbf{S}(0) = 0$.\n",
    "\n",
    "2. **Problem with Standard ANN Models**:\n",
    "    - Standard regression ANN models may violate this property even if the training data is consistent.\n",
    "    - Such a violation causes issues in numerical simulations, leading to deformation without load or prescribed displacement.\n",
    "\n",
    "3. **Proposed Solution**:\n",
    "    - To ensure consistency, the strain energy density function is represented as a combination of the ANN model and a linear correction term:\n",
    "    $$\n",
    "    W(\\widehat{\\mathbf{E}}) = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}}) + \\mathbf{h} \\cdot \\widehat{\\mathbf{E}}\n",
    "    $$\n",
    "    - The stress is derived as:\n",
    "    $$\n",
    "    \\widehat{\\mathbf{S}}(\\widehat{\\mathbf{E}}) = \\frac{\\partial W}{\\partial \\widehat{\\mathbf{E}}}(\\widehat{\\mathbf{E}}) = \\frac{\\partial \\mathcal{N}_\\theta}{\\partial \\widehat{\\mathbf{E}}}(\\widehat{\\mathbf{E}}) + \\mathbf{h}\n",
    "    $$\n",
    "\n",
    "4. **Ensuring Consistency**:\n",
    "    - By choosing $\\mathbf{h} = -\\frac{\\partial \\mathcal{N}_\\theta}{\\partial \\hat{\\mathbf{E}}}(0)$, the desired consistency property is guaranteed.\n",
    "    - This approach ensures that the ANN preserves both hyperelasticity and convexity of the strain energy density function.\n",
    "\n",
    "5. **Embedding Correction in Training**:\n",
    "    - To maintain accuracy, the correction should be embedded directly into the training procedure, rather than applied afterward.\n",
    "    - This is done by modifying the loss function:\n",
    "    $$\n",
    "    \\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left( \\frac{\\frac{\\partial \\mathcal{N}_\\beta}{\\partial \\widehat{\\mathrm{E}}_k}\\left(\\widehat{\\mathbf{E}}^{(m)}\\right) - \\frac{\\partial \\mathcal{N}_\\beta}{\\partial \\widehat{\\mathrm{E}}_k}(0) - \\widehat{S}_k^{(m)}}{\\sigma_k} \\right)^2\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
