{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Convex Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "a new neural network architecture: input convex neural network (ICNN).\n",
    "\n",
    "scalar-valued neural networks $f(x, y ; \\theta)$\n",
    "$x$ and $y$ denotes inputs to the function \n",
    "$\\theta$ denotes the parameters, built in such a way that the network is convex in (a subset of) inputs $y \n",
    "\n",
    "fundamental benefit: we can optimize over the convex inputs to the network given some fixed value for other inputs. That is, given some fixed $x$ (and possibly some fixed elements of[^0]$y$ ) we can globally and efficiently (because the problem is convex) solve the optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\underset{y}{\\operatorname{argmin}} f(x, y ; \\theta) \\tag{1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "we can perform inference in the network via optimization. \n",
    "\n",
    "instead of making predictions in a neural network via a purely feedforward process,\n",
    "we can make predictions by optimizing a scalar function (which effectively plays the role of an energy function) over some inputs to the function given \n",
    "\n",
    "potential use cases for these networks.\n",
    "\n",
    "Structured prediction: \n",
    "\n",
    "Given (typically high-dimensional) structured input and output spaces $\\mathcal{X} \\times \\mathcal{Y}$, we can build a network over $(x, y)$ pairs that encodes the energy function for this pair, following typical energy-based learning formalisms (LeCun et al., 2006). \n",
    "\n",
    "Prediction involves finding the $y \\in \\mathcal{Y}$ that minimizes the energy for a given $x$, which is exactly the argmin problem in (1). \n",
    "\n",
    "\n",
    "In our setting, assuming that $\\mathcal{Y}$ is a convex space (a common assumption in structured prediction), this optimization problem is convex. \n",
    "\n",
    "This is similar in nature to the structured prediction energy networks (SPENs) (Belanger \\& McCallum, 2016), which also use deep networks over the input and output spaces, with the difference being that in our setting $f$ is convex in $y$, so the optimization can be performed globally.\n",
    "\n",
    "\n",
    "Data imputation:\n",
    "\n",
    "if we are given some space $\\mathcal{Y}$ we can learn a network $f(y ; \\theta)$ (removing the additional $x$ inputs, though these can be added as well) that, given an example with some subset $\\mathcal{I}$ missing, imputes the likely values of these variables by solving the optimization problem as above $\\hat{y}_{\\mathcal{I}}=\\operatorname{argmin}_{y_{\\mathcal{I}}} f\\left(y_{\\mathcal{I}}, y_{\\overline{\\mathcal{I}}} ; \\theta\\right)$ This could be used\n",
    "\n",
    "[^1]e.g., in image inpainting where the goal is to fill in some arbitrary set of missing pixels given observed ones.\n",
    "\n",
    "Continuous action reinforcement learning Given a reinforcement learning problem with potentially continuous state and action spaces $\\mathcal{S} \\times \\mathcal{A}$, we can model the (negative) $Q$ function, $-Q(s, a ; \\theta)$ as an input convex neural network. In this case the action selection procedure can be formulated as a convex optimization problem $a^{\\star}(s)=$ $\\operatorname{argmin}_{a}-Q(s, a ; \\theta)$.\n",
    "\n",
    "This paper lays the foundation for optimization, inference, and learning in these input convex models, and explores their performance in the applications above. Our main contributions are: we propose the ICNN architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convex neural network architectures\n",
    "\n",
    "chief claim: the class of (full and partial) input convex models is rich \n",
    "\n",
    "### 3.1. Fully input convex neural networks\n",
    "\n",
    "consider a fully convex, $k$-layer, fully connected ICNN that we call a FICNN and is shown in Figure 1. This model defines a neural network over the input $y$ (i.e., omitting any $x$ term in this function) using the architecture for $i=0, \\ldots, k-1$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "z_{i+1}=g_{i}\\left(W_{i}^{(z)} z_{i}+W_{i}^{(y)} y+b_{i}\\right), \\quad f(y ; \\theta)=z_{k} \\tag{2}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "where $z_{i}$ denotes the layer activations (with $z_{0}, W_{0}^{(z)} \\equiv 0$ ), $\\theta=\\left\\{W_{0: k-1}^{(y)}, W_{1: k-1}^{(z)}, b_{0: k-1}\\right\\}$ are the parameters, and $g_{i}$ are non-linear activation functions. The central result on convexity of the network is the following:\n",
    "Proposition 1. The function $f$ is convex in $y$ provided that all $W_{1: k-1}^{(z)}$ are non-negative, and all functions $g_{i}$ are convex and non-decreasing.\n",
    "\n",
    "proof:nonnegative sums of convex functions are also convex and that the composition of a convex and convex non-decreasing function is also convex (see e.g. Boyd \\& Vandenberghe (2004, 3.2.4)). The constraint that the $g_{i}$ be convex nondecreasing is not particularly restrictive, as current nonlinear activation units like the rectified linear unit or maxpooling unit already satisfy this constraint. The constraint that the $W^{(z)}$ terms be non-negative is somewhat restrictive, but because the bias terms and $W^{(y)}$ terms can be negative, the network still has substantial representation power, as we will shortly demonstrate empirically.\n",
    "One notable addition in the ICNN are the \"passthrough\" layers that directly connect the input $y$ to hidden units in\n",
    "![](https://cdn.mathpix.com/cropped/2024_11_05_2df55116d7c46344fa72g-03.jpg?height=324&width=709&top_left_y=212&top_left_x=1109)\n",
    "\n",
    "Figure 2. A partially input convex neural network (PICNN).\n",
    "deeper layers. Such layers are unnecessary in traditional feedforward networks because previous hidden units can always be mapped to subsequent hidden units with the identity mapping; however, for ICNNs, the non-negativity constraint subsequent $W^{(z)}$ weights restricts the allowable use of hidden units that mirror the identity mapping, and so we explicitly include this additional passthrough. Some passthrough layers have been recently explored in the deep residual networks (He et al., 2015) and densely connected convolutional networks (Huang et al., 2016), though these differ from those of an ICNN as they pass through hidden layers deeper in the network, whereas to maintain convexity our passthrough layers can only apply to the input directly.\n",
    "Other linear operators like convolutions can be included in ICNNs without changing the convexity properties. Indeed, modern feedforward architectures such as AlexNet (Krizhevsky et al., 2012), VGG (Simonyan \\& Zisserman, 2014), and GoogLeNet (Szegedy et al., 2015) with ReLUs (Nair \\& Hinton, 2010) can be made input convex with Proposition 1. In the experiment that follow, we will explore ICNNs with both fully connected and convolutional layers, and we provide more detail about these additional architectures in Section A of the supplement.\n",
    "\n",
    "### 3.2. Partially input convex architectures\n",
    "\n",
    "The FICNN provides joint convexity over the entire input to the function, which indeed may be a restriction on the allowable class of models. Furthermore, this full joint convexity is unnecessary in settings like structured prediction where the neural network is used to build a joint model over an input and output example space and only convexity over the outputs is necessary.\n",
    "In this section we propose an extension to the pure FICNN, the partially input convex neural network (PICNN), that is convex over only some inputs to the network (in general ICNNs will refer to this new class). As we will show, these networks generalize both traditional feedforward networks and FICNNs, and thus provide substantial representational benefits. We define a PICNN to be a network over $(x, y)$ pairs $f(x, y ; \\theta)$ where $f$ is convex in $y$ but not convex in $x$. Figure 2 illustrates one potential $k$-layer PICNN architec-\n",
    "ture defined by the recurrences\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "u_{i+1} & =\\tilde{g}_{i}\\left(\\tilde{W}_{i} u_{i}+\\tilde{b}_{i}\\right) \\\\\n",
    "z_{i+1} & =g_{i}\\left(W_{i}^{(z)}\\left(z_{i} \\circ\\left[W_{i}^{(z u)} u_{i}+b_{i}^{(z)}\\right]_{+}\\right)+\\right. \\\\\n",
    "& \\left.W_{i}^{(y)}\\left(y \\circ\\left(W_{i}^{(y u)} u_{i}+b_{i}^{(y)}\\right)\\right)+W_{i}^{(u)} u_{i}+b_{i}\\right) \\\\\n",
    "f(x, y ; \\theta) & =z_{k}, u_{0}=x \\tag{3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $u_{i} \\in \\mathbb{R}^{n_{i}}$ and $z_{i} \\in \\mathbb{R}^{m_{i}}$ denote the hidden units for the \" $x$-path\" and \" $y$-path\", where $y \\in \\mathbb{R}^{p}$, and where - denotes the Hadamard product, the elementwise product between two vectors. The crucial element here is that unlike the FICNN, we only need the $W^{(z)}$ terms to be nonnegative, and we can introduce arbitrary products between the $u_{i}$ hidden units and the $z_{i}$ hidden units. The following proposition highlights the representational power of the PICNN.\n",
    "Proposition 2. A PICNN network with $k$ layers can represent any FICNN with $k$ layers and any purely feedforward network with $k$ layers.\n",
    "\n",
    "Proof. To recover a FICNN we simply set the weights over the entire $x$ path to be zero and set $b^{(z)}=b^{(y)}=1$. We can recover a feedforward network by noting that a traditional feedforward network $\\hat{f}(x ; \\theta)$ where $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$, can be viewed as a network with an inner product $f(x ; \\theta)^{T} y$ in its last layer (see e.g. (LeCun et al., 2006) for more details). Thus, a feedforward network can be represented as a PICNN by setting the $x$ path to be exactly the feedforward component, then having the $y$ path be all zero except $W_{k-1}^{(y u)}=I$ and $W_{k-1}^{(y)}=1^{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class StandardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        self.activation = nn.ReLU()  # Non-linear activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ICNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        self.activation = nn.Softplus()\n",
    "\n",
    "        # Enforce non-negative weights (e.g., with ReLU on weights)\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            layer.weight.data = torch.abs(layer.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        return self.fc3(x) + x**2  # Add convex term x^2 for guaranteed convexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NN Output: -0.37659865617752075, Gradient: 22.6495361328125\n",
      "ICNN Output: 57.33120346069336, Gradient: 22.6495361328125\n"
     ]
    }
   ],
   "source": [
    "# Ensure x is a scalar input for simplicity in this example\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Instantiate models\n",
    "nn_model = StandardNN()\n",
    "icnn_model = ICNN()\n",
    "\n",
    "# Forward pass through standard NN\n",
    "y_nn = nn_model(x)\n",
    "y_nn_scalar = y_nn.sum()  # Ensure scalar output\n",
    "y_nn_scalar.backward()\n",
    "grad_nn = x.grad.data\n",
    "\n",
    "# Clear gradients\n",
    "x.grad.data.zero_()\n",
    "\n",
    "# Forward pass through ICNN\n",
    "y_icnn = icnn_model(x)\n",
    "y_icnn_scalar = y_icnn.sum()  # Ensure scalar output by summing all elements\n",
    "y_icnn_scalar.backward()\n",
    "grad_icnn = x.grad.data\n",
    "\n",
    "print(f\"Standard NN Output: {y_nn_scalar.item()}, Gradient: {grad_nn.item()}\")\n",
    "print(f\"ICNN Output: {y_icnn_scalar.item()}, Gradient: {grad_icnn.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
