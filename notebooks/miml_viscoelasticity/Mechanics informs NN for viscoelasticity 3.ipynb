{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n",
    "from scipy.interpolate import CubicSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for the viscoelastic model\n",
    "A = 200  # Elastic stiffness matrix in MPa\n",
    "Q = 10   # Kernel decay rate\n",
    "D = 50   # Damping coefficient in MPaÂ·s\n",
    "dt = 0.01  # Time step\n",
    "\n",
    "# Define time and custom strain points\n",
    "total_time = 3\n",
    "time = np.arange(0, total_time, dt)\n",
    "custom_strain_points = [0, 0.05, -0.05, 0.1, -0.1, 0.15, -0.15, 0]\n",
    "custom_time_points = np.linspace(0, total_time, len(custom_strain_points))\n",
    "\n",
    "# Use cubic spline interpolation to create a smooth strain path\n",
    "cubic_spline = CubicSpline(custom_time_points, custom_strain_points)\n",
    "strain = cubic_spline(time)\n",
    "\n",
    "# Compute the strain rate (dE/dt) using the derivative of the spline\n",
    "strain_rate = cubic_spline(time, 1)  # First derivative of the strain path\n",
    "\n",
    "# Initialize stress array\n",
    "stress = np.zeros_like(time)\n",
    "\n",
    "# Compute stress using the convolution integral\n",
    "for i in range(1, len(time)):\n",
    "    # Elastic stress component\n",
    "    elastic_stress = A * strain[i]\n",
    "    \n",
    "    # Viscoelastic stress component\n",
    "    viscoelastic_stress = 0\n",
    "    for j in range(i):\n",
    "        kernel = np.exp(-Q * (time[i] - time[j]))  # Decaying kernel\n",
    "        viscoelastic_stress += kernel * D * strain_rate[j] * dt  # Convolution for viscoelastic contribution\n",
    "    \n",
    "    # Total stress is the sum of elastic and viscoelastic components\n",
    "    stress[i] = elastic_stress + viscoelastic_stress\n",
    "\n",
    "# Plot the strain-time curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time, strain, label=\"Strain vs Time\", color='b', linewidth=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Strain\")\n",
    "plt.title(\"Strain-Time Curve (Smooth Path in 3 Seconds)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the stress-strain curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(strain, stress, label=\"Stress vs Strain\", color='r', linewidth=2)\n",
    "plt.xlabel(\"Strain\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Stress-Strain Curve (Viscoelastic Material)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Convert to torch tensors for training\n",
    "strain_tensor = torch.tensor(strain, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)\n",
    "stress_tensor = torch.tensor(stress, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)\n",
    "time_tensor = torch.tensor(time, dtype=torch.float32).unsqueeze(1)      # Shape (N, 1)\n",
    "\n",
    "# Combine time, strain, and stress into a single dataset\n",
    "dataset = TensorDataset(time_tensor, strain_tensor, stress_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SoftplusSquared activation function\n",
    "class SoftplusSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.log(1 + torch.exp(x)) ** 2\n",
    "\n",
    "activations = {\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    # \"LeakyReLU\": nn.LeakyReLU(),\n",
    "    # \"ELU\": nn.ELU(),\n",
    "    # \"SELU\": nn.SELU(),\n",
    "    \"Softplus\": nn.Softplus(),\n",
    "    # \"Sigmoid\": nn.Sigmoid(),\n",
    "    # \"Tanh\": nn.Tanh(),\n",
    "    \"SoftplusSquared\": SoftplusSquared()}\n",
    "\n",
    "x_values = torch.linspace(-2, 2, 100)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for name, activation in activations.items():\n",
    "    y_values = activation(x_values).detach().numpy()\n",
    "    if name == \"SoftplusSquared\":\n",
    "        plt.plot(x_values.numpy(), y_values, label=name, linewidth=2.5)\n",
    "    else:\n",
    "        plt.plot(x_values.numpy(), y_values, label=name)\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.title(\"Comparison of PyTorch Activation Functions (SoftplusSquared Highlighted)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Linear layer with non-negative weights for ICNN\n",
    "class NonNegativeLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(NonNegativeLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # Apply ReLU to the weights to ensure non-negativity\n",
    "        self.linear.weight.data = torch.abs(self.linear.weight.data)\n",
    "        self.linear.bias.data = torch.abs(self.linear.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ReLU on weights to maintain non-negativity\n",
    "        return torch.nn.functional.linear(x, torch.relu(self.linear.weight), self.linear.bias)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the FICNN model for W_{(\\theta)} with 1D strain\n",
    "class PotentialW_FICNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialW_FICNN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            NonNegativeLinear(1, 64),  # Input layer with non-negative weights\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Hidden layer 1 with non-negative weights\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Hidden layer 2 with non-negative weights\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 1)   # Output layer with non-negative weights\n",
    "        )\n",
    "# passthrough ****\n",
    "    def forward(self, strain):\n",
    "        # Compute N_theta(strain) using the FICNN\n",
    "        # strain=strain_mean- SDv\n",
    "        n_theta = self.fc(strain)\n",
    "        # n_theta= n_theta* SDV_thets\n",
    "    \n",
    "        # Compute the gradient term d(N_theta)/d(strain) at strain = 0\n",
    "        strain_zero = torch.tensor([0.0], requires_grad=True)\n",
    "        n_theta_zero = self.fc(strain_zero)\n",
    "        gradient = torch.autograd.grad(n_theta_zero, strain_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute W(theta) as N_theta - gradient * strain\n",
    "        W_theta = n_theta - gradient * strain\n",
    "        return W_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FICNN for V_{(\\Phi)} with learned parameters C1 and C2\n",
    "class PotentialV_FICNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialV_FICNN, self).__init__()\n",
    "        # Define the neural network for M_{Phi} with 4 hidden layers\n",
    "        self.fc = nn.Sequential(\n",
    "            NonNegativeLinear(1, 64),  # Input layer with non-negative weights\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Hidden layer 1 with non-negative weights\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Hidden layer 2 with non-negative weights\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 1)   # Output layer with non-negative weights\n",
    "        )\n",
    "        # Define learnable parameters C1 and C2\n",
    "        self.C1 = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n",
    "        self.C2 = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n",
    "\n",
    "    def forward(self, strain, alpha):\n",
    "        # Calculate the input to the network: delta = C1 * strain + C2 * alpha\n",
    "        delta = self.C1 * strain + self.C2 * alpha\n",
    "\n",
    "        # Compute M_phi(delta) using the FICNN\n",
    "        m_phi = self.fc(delta)\n",
    "\n",
    "        # Compute the gradient term d(M_phi)/d(delta) at delta = 0\n",
    "        delta_zero = torch.tensor([0.0], requires_grad=True)\n",
    "        m_phi_zero = self.fc(delta_zero)\n",
    "        gradient = torch.autograd.grad(m_phi_zero, delta_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute V_phi as M_phi - gradient * delta\n",
    "        V_phi = m_phi - gradient * delta\n",
    "        return V_phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FICNN for G_{(\\Gamma)} with full input convexity over beta and E\n",
    "class PotentialG_FICNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialG_FICNN, self).__init__()\n",
    "        \n",
    "        # Define the fully convex network with respect to both beta and E\n",
    "        self.fc = nn.Sequential(\n",
    "            NonNegativeLinear(2, 64),  # Convex layer with respect to beta and E\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Convex hidden layer 1\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Convex hidden layer 2\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 1)   # Output layer, convex with respect to beta and E\n",
    "        )\n",
    "\n",
    "    def forward(self, beta, strain):\n",
    "        # Concatenate beta and strain as the input\n",
    "        input_data = torch.cat([beta, strain], dim=-1)  # Shape: (batch_size, 2)\n",
    "\n",
    "        # Compute D_gamma(beta, epsilon) using the FICNN\n",
    "        d_gamma = self.fc(input_data)\n",
    "\n",
    "        # Compute the gradient term d(D_gamma)/d(beta) at beta = 0 for linear correction\n",
    "        beta_zero = torch.tensor([0.0], requires_grad=True).view(1, 1)\n",
    "        strain_fixed = strain.view(1, 1)\n",
    "        input_zero = torch.cat([beta_zero, strain_fixed], dim=-1)\n",
    "        d_gamma_zero = self.fc(input_zero)\n",
    "\n",
    "        # Calculate the gradient with respect to beta at beta = 0\n",
    "        gradient = torch.autograd.grad(d_gamma_zero, beta_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute G_gamma as D_gamma - gradient * beta\n",
    "        G_gamma = d_gamma - gradient * beta\n",
    "        return G_gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the PICNN for G_{(\\Gamma)} with convexity in beta and not in E\n",
    "class PotentialG_PICNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialG_PICNN, self).__init__()\n",
    "        \n",
    "        # Define beta-path (convex with respect to beta)\n",
    "        self.beta_path = nn.Sequential(\n",
    "            NonNegativeLinear(1, 64),  # Convex layer with respect to beta\n",
    "            SoftplusSquared(),\n",
    "            NonNegativeLinear(64, 64), # Convex hidden layer\n",
    "            SoftplusSquared()\n",
    "        )\n",
    "\n",
    "        # Define E-path (non-convex with respect to E)\n",
    "        self.epsilon_path = nn.Sequential(\n",
    "            nn.Linear(1, 64),          # Standard linear layer, no non-negativity constraint\n",
    "            SoftplusSquared(),\n",
    "            nn.Linear(64, 64),  # Another non-convex hidden layer\n",
    "            SoftplusSquared()\n",
    "        )\n",
    "\n",
    "        # Final layer combining both paths, convex with respect to beta\n",
    "        self.output_layer = NonNegativeLinear(64, 1)  # Ensure convexity with respect to beta     # recheck\n",
    "\n",
    "    def forward(self, beta, strain):\n",
    "        # Compute the output of the beta path (convex path)\n",
    "        beta_out = self.beta_path(beta)\n",
    "\n",
    "        # Compute the output of the epsilon path (non-convex path)\n",
    "        epsilon_out = self.epsilon_path(strain)\n",
    "\n",
    "        # Combine the outputs of both paths\n",
    "        combined = beta_out * epsilon_out  # Hadamard product allows interaction between beta and epsilon paths\n",
    "\n",
    "        # Final output layer that ensures convexity with respect to beta\n",
    "        d_gamma = self.output_layer(combined)\n",
    "\n",
    "        # Compute the gradient term d(D_gamma)/d(beta) at beta = 0 for linear correction\n",
    "        beta_zero = torch.tensor([0.0], requires_grad=True).view(1, -1)\n",
    "        beta_out_zero = self.beta_path(beta_zero)\n",
    "        epsilon_out_fixed = self.epsilon_path(strain)  # epsilon path evaluated at given strain\n",
    "        combined_zero = beta_out_zero * epsilon_out_fixed\n",
    "        d_gamma_zero = self.output_layer(combined_zero)\n",
    "\n",
    "        # Calculate the gradient with respect to beta at beta = 0\n",
    "        gradient = torch.autograd.grad(d_gamma_zero, beta_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute G_gamma as D_gamma - gradient * beta\n",
    "        G_gamma = d_gamma - gradient * beta\n",
    "        return G_gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaSurrogate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlphaSurrogate, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),           # Input layer\n",
    "            SoftplusSquared(),\n",
    "            nn.Linear(64, 64),          # Hidden layer 1\n",
    "            SoftplusSquared(),\n",
    "            nn.Linear(64, 64),          # Hidden layer 2\n",
    "            SoftplusSquared(),\n",
    "            nn.Linear(64, 64),          # Hidden layer 3\n",
    "            SoftplusSquared(),\n",
    "            nn.Linear(64, 1)            # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.fc(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete viscoelastic model with alpha surrogate\n",
    "class ViscoelasticModelWithAlpha(nn.Module):\n",
    "    def __init__(self, W_model, V_model, G_model, alpha_surrogate):\n",
    "        super(ViscoelasticModelWithAlpha, self).__init__()\n",
    "        self.W_model = W_model\n",
    "        self.V_model = V_model\n",
    "        self.G_model = G_model\n",
    "        self.alpha_surrogate = alpha_surrogate\n",
    "\n",
    "    def forward(self, strain, alpha, t):\n",
    "        # Step 1: Compute internal stress beta using V_model\n",
    "        V = self.V_model(strain, alpha)\n",
    "        V_sum = V.sum()\n",
    "        beta = -grad(V_sum, alpha, create_graph=True)[0]\n",
    "\n",
    "        # Step 2: Evolve alpha using G_model and surrogate\n",
    "        alpha_evolution = self.G_model(strain, beta)\n",
    "        alpha_surrogate = self.alpha_surrogate(t)\n",
    "\n",
    "        # Step 3: Compute total stress using W_model and V_model\n",
    "        W = self.W_model(strain)\n",
    "        dW_dE = grad(W.sum(), strain, create_graph=True)[0]\n",
    "        dV_dE = grad(V_sum, strain, create_graph=True)[0]\n",
    "        total_stress = dW_dE + dV_dE\n",
    "\n",
    "        return total_stress, alpha_evolution, alpha_surrogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmented loss function with penalty\n",
    "def augmented_loss(predicted_stress, true_stress, alpha_surrogate, alpha_evolution, lambda_penalty=0.1):\n",
    "    mse_loss = nn.MSELoss()(predicted_stress, true_stress)\n",
    "    penalty_loss = lambda_penalty * torch.mean((alpha_surrogate - alpha_evolution) ** 2)\n",
    "    total_loss = mse_loss + penalty_loss\n",
    "    return total_loss\n",
    "\n",
    "# Instantiate the models\n",
    "W_model = PotentialW_FICNN()\n",
    "V_model = PotentialV_FICNN()\n",
    "G_model = PotentialG_PICNN()\n",
    "alpha_surrogate = AlphaSurrogate()\n",
    "\n",
    "# Instantiate the viscoelastic model with the alpha surrogate\n",
    "viscoelastic_model = ViscoelasticModelWithAlpha(W_model, V_model, G_model, alpha_surrogate)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(viscoelastic_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# from Faisal:\n",
    "# params = list(model_M.parameters()) + list(model_N.parameters()) + list(model_D.parameters()) + list(model_alpha.parameters())\n",
    "#\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    # Loop through the data loader batches\n",
    "    for batch_idx, (strain_batch, true_stress_batch) in enumerate(data_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure strain_batch requires grad for computing potentials W, V, and G\n",
    "        strain_batch.requires_grad_(True)\n",
    "\n",
    "        # Initialize alpha with requires_grad=True so we can compute gradients for alpha evolution\n",
    "        alpha = torch.zeros_like(strain_batch, requires_grad=True)\n",
    "\n",
    "        # Forward pass: compute predicted stress, alpha evolution, and surrogate alpha\n",
    "        predicted_stress, alpha_evolution, alpha_surrogate_output = viscoelastic_model(strain_batch, alpha, t)\n",
    "\n",
    "        # Compute the augmented loss (MSE + penalty for alpha evolution)\n",
    "        loss = augmented_loss(predicted_stress, true_stress_batch, alpha_surrogate_output, alpha_evolution)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for monitoring every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and compare with real stress\n",
    "viscoelastic_model.eval()\n",
    "\n",
    "# Forward pass to get predictions on the full dataset\n",
    "with torch.no_grad():\n",
    "    # Initialize alpha for inference without requires_grad=True\n",
    "    alpha = torch.zeros_like(strain_tensor)\n",
    "\n",
    "    # Forward pass through the model to get predictions without computing gradients\n",
    "    predicted_stress_list = []\n",
    "\n",
    "      \n",
    "    # Iterate over each strain value in the dataset\n",
    "    for i in range(len(strain_tensor)):\n",
    "        strain = strain_tensor[i].unsqueeze(0)  # Shape (1, 1)\n",
    "        alpha_i = alpha[i].unsqueeze(0)  # Shape (1, 1)\n",
    "\n",
    "        # Use the model to compute stress using the learned potentials without needing gradients\n",
    "        W_output = W_model(strain)\n",
    "        V_output = V_model(strain, alpha_i)\n",
    "\n",
    "        # Here we directly use the outputs from W_model and V_model as stress approximation\n",
    "        total_stress = W_output + V_output\n",
    "\n",
    "        # Collect the predicted stress\n",
    "        predicted_stress_list.append(total_stress.item())\n",
    "\n",
    "    # Convert the list of predicted stresses to a numpy array for plotting\n",
    "    predicted_stress = np.array(predicted_stress_list)\n",
    "\n",
    "# Convert strain and stress to numpy arrays for easier plotting\n",
    "strain_np = strain_tensor.squeeze().numpy()  # Shape (N,)\n",
    "stress_np = stress_tensor.squeeze().numpy()  # Shape (N,)\n",
    "\n",
    "# Plot the real vs predicted stress vs time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, stress_np, label=\"Real Stress\", color='b', linewidth=2)\n",
    "plt.plot(time, predicted_stress, label=\"Predicted Stress\", color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Real vs Predicted Stress (Time Domain)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the real vs predicted stress-strain curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(strain_np, stress_np, label=\"Real Stress-Strain\", color='b', linewidth=2)\n",
    "plt.plot(strain_np, predicted_stress, label=\"Predicted Stress-Strain\", color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel(\"Strain\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Real vs Predicted Stress-Strain Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
