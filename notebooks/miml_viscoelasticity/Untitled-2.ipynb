{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the viscoelastic model\n",
    "A = 200  # Elastic stiffness matrix in MPa\n",
    "Q = 10   # Kernel decay rate\n",
    "D = 50   # Damping coefficient in MPaÂ·s\n",
    "dt = 0.01  # Time step\n",
    "\n",
    "# Time array\n",
    "total_time = 3  # Total time for custom strain path (3 seconds)\n",
    "time = np.arange(0, total_time, dt)  # Time array\n",
    "\n",
    "# Define custom strain points and corresponding time points\n",
    "custom_strain_points = [0, 0.05, -0.05, 0.1, -0.1, 0.15, -0.15, 0]\n",
    "custom_time_points = np.linspace(0, total_time, len(custom_strain_points))\n",
    "\n",
    "# Interpolate to create the strain path\n",
    "strain = np.interp(time, custom_time_points, custom_strain_points)\n",
    "\n",
    "# Strain rate (dE/dt) using numerical differentiation\n",
    "strain_rate = np.gradient(strain, dt)\n",
    "\n",
    "# Initialize stress array for storing computed stress values\n",
    "stress = np.zeros_like(time)\n",
    "\n",
    "# Compute stress using the convolution integral\n",
    "for i in range(1, len(time)):\n",
    "    # Elastic stress component\n",
    "    elastic_stress = A * strain[i]\n",
    "    \n",
    "    # Viscoelastic stress component\n",
    "    viscoelastic_stress = 0\n",
    "    for j in range(i):\n",
    "        kernel = np.exp(-Q * (time[i] - time[j]))  # Decaying kernel\n",
    "        viscoelastic_stress += kernel * D * strain_rate[j] * dt  # Convolution for viscoelastic contribution\n",
    "    \n",
    "    # Total stress is the sum of elastic and viscoelastic components\n",
    "    stress[i] = elastic_stress + viscoelastic_stress\n",
    "\n",
    "# Plot the strain-time curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time, strain, label=\"Strain vs Time\", color='b', linewidth=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Strain\")\n",
    "plt.title(\"Strain-Time Curve (Custom Path in 3 Seconds)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the stress-strain curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(strain, stress, label=\"Stress vs Strain\", color='r', linewidth=2)\n",
    "plt.xlabel(\"Strain\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Stress-Strain Curve (Viscoelastic Material)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Convert to torch tensors for training\n",
    "strain_tensor = torch.tensor(strain, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)\n",
    "stress_tensor = torch.tensor(stress, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(strain_tensor, stress_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialW, self).__init__()\n",
    "        # Define the neural network layers for learning W (N_theta)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Input: strain (E)\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, strain):\n",
    "        # Compute the neural network output N_theta(E)\n",
    "        N_theta = self.fc(strain)\n",
    "        \n",
    "        # Compute the gradient of N_theta with respect to strain at E = 0\n",
    "        # This is necessary to enforce consistency\n",
    "        strain_zero = torch.zeros_like(strain)  # Zero strain for consistency condition\n",
    "        N_theta_grad_at_zero = torch.autograd.functional.jacobian(lambda x: self.fc(x), strain_zero).squeeze()\n",
    "        \n",
    "        # Implement W(E) = N_theta(E) - N_theta_grad_at_zero * E\n",
    "        W_theta = N_theta - N_theta_grad_at_zero * strain\n",
    "        \n",
    "        return W_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialW(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialW, self).__init__()\n",
    "        # Define the neural network layers for learning W (N_theta)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Input: strain (E)\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, strain):\n",
    "        # Compute the neural network output N_theta(E)\n",
    "        N_theta = self.fc(strain)\n",
    "        \n",
    "        # Compute the gradient of N_theta with respect to strain at E = 0\n",
    "        # This is necessary to enforce consistency\n",
    "        strain_zero = torch.zeros_like(strain)  # Zero strain for consistency condition\n",
    "        N_theta_grad_at_zero = torch.autograd.functional.jacobian(lambda x: self.fc(x), strain_zero).squeeze()\n",
    "        \n",
    "        # Implement W(E) = N_theta(E) - N_theta_grad_at_zero * E\n",
    "        W_theta = N_theta - N_theta_grad_at_zero * strain\n",
    "        \n",
    "        return W_theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialV, self).__init__()\n",
    "        # Define the neural network layers for learning V (M_Phi)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Input will be delta = C1 * E + C2 * alpha\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        # Initialize C1 and C2 as learnable parameters (4th-order tensors)\n",
    "        self.C1 = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n",
    "        self.C2 = nn.Parameter(torch.tensor([1.0]), requires_grad=True)\n",
    "    \n",
    "    def forward(self, strain, alpha):\n",
    "        # Compute delta = C1 * E + C2 * alpha\n",
    "        delta = self.C1 * strain + self.C2 * alpha\n",
    "        \n",
    "        # Compute the neural network output M_Phi(delta)\n",
    "        M_Phi = self.fc(delta)\n",
    "        \n",
    "        # Compute the gradient of M_Phi with respect to delta at delta = 0\n",
    "        delta_zero = torch.zeros_like(delta)  # Zero delta for consistency condition\n",
    "        M_Phi_grad_at_zero = torch.autograd.functional.jacobian(lambda x: self.fc(x), delta_zero).squeeze()\n",
    "        \n",
    "        # Implement V(E, alpha) = M_Phi(delta) - M_Phi_grad_at_zero * delta\n",
    "        V_Phi = M_Phi - M_Phi_grad_at_zero * delta\n",
    "        \n",
    "        return V_Phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotentialG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PotentialG, self).__init__()\n",
    "        # Define the neural network layers for learning G (D_r)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Input: beta (internal stress)\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, beta, strain):\n",
    "        # Compute the neural network output D_r(beta, E)\n",
    "        D_r = self.fc(beta)\n",
    "        \n",
    "        # Compute the gradient of D_r with respect to beta at beta = 0\n",
    "        beta_zero = torch.zeros_like(beta)  # Zero beta for consistency condition\n",
    "        D_r_grad_at_zero = torch.autograd.functional.jacobian(lambda x: self.fc(x), beta_zero).squeeze()\n",
    "        \n",
    "        # Implement G(beta, E) = D_r(beta, E) - D_r_grad_at_zero * beta\n",
    "        G_sigma = D_r - D_r_grad_at_zero * beta\n",
    "        \n",
    "        return G_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
