{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_pi_predictor_2 import *\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.1A\\Pi_data_10C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.2A\\Pi_data_10C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.3A\\Pi_data_10C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.4A\\Pi_data_10C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.5A\\Pi_data_10C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.6A\\Pi_data_10C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.7A\\Pi_data_10C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.8A\\Pi_data_10C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_0.9A\\Pi_data_10C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.0A\\Pi_data_10C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.1A\\Pi_data_10C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.2A\\Pi_data_10C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.3A\\Pi_data_10C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.4A\\Pi_data_10C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.5A\\Pi_data_10C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.6A\\Pi_data_10C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.7A\\Pi_data_10C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.8A\\Pi_data_10C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_1.9A\\Pi_data_10C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\10C_2.0A\\Pi_data_10C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.1A\\Pi_data_1C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.2A\\Pi_data_1C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.3A\\Pi_data_1C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.4A\\Pi_data_1C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.5A\\Pi_data_1C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.6A\\Pi_data_1C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.7A\\Pi_data_1C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.8A\\Pi_data_1C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_0.9A\\Pi_data_1C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.0A\\Pi_data_1C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.1A\\Pi_data_1C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.2A\\Pi_data_1C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.3A\\Pi_data_1C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.4A\\Pi_data_1C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.5A\\Pi_data_1C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.6A\\Pi_data_1C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.7A\\Pi_data_1C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.8A\\Pi_data_1C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_1.9A\\Pi_data_1C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\1C_2.0A\\Pi_data_1C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2.5C_1.45A_40S\\Pi_data_2.5C_1.45A_40S.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.1A\\Pi_data_2C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.2A\\Pi_data_2C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.3A\\Pi_data_2C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.4A\\Pi_data_2C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.5A\\Pi_data_2C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.6A\\Pi_data_2C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.7A\\Pi_data_2C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.8A\\Pi_data_2C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_0.9A\\Pi_data_2C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.0A\\Pi_data_2C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.1A\\Pi_data_2C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.2A\\Pi_data_2C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.3A\\Pi_data_2C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.4A\\Pi_data_2C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.5A\\Pi_data_2C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.6A\\Pi_data_2C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.7A\\Pi_data_2C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.8A\\Pi_data_2C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_1.9A\\Pi_data_2C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\2C_2.0A\\Pi_data_2C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.1A\\Pi_data_3C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.2A\\Pi_data_3C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.3A\\Pi_data_3C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.4A\\Pi_data_3C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.5A\\Pi_data_3C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.6A\\Pi_data_3C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.7A\\Pi_data_3C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.8A\\Pi_data_3C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_0.9A\\Pi_data_3C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.0A\\Pi_data_3C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.1A\\Pi_data_3C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.2A\\Pi_data_3C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.3A\\Pi_data_3C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.4A\\Pi_data_3C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.5A\\Pi_data_3C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.6A\\Pi_data_3C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.7A\\Pi_data_3C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.8A\\Pi_data_3C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_1.9A\\Pi_data_3C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\3C_2.0A\\Pi_data_3C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.1A\\Pi_data_4C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.2A\\Pi_data_4C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.3A\\Pi_data_4C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.4A\\Pi_data_4C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.5A\\Pi_data_4C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.6A\\Pi_data_4C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.7A\\Pi_data_4C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.8A\\Pi_data_4C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_0.9A\\Pi_data_4C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.0A\\Pi_data_4C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.1A\\Pi_data_4C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.2A\\Pi_data_4C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.3A\\Pi_data_4C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.4A\\Pi_data_4C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.5A\\Pi_data_4C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.6A\\Pi_data_4C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.7A\\Pi_data_4C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.8A\\Pi_data_4C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_1.9A\\Pi_data_4C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\4C_2.0A\\Pi_data_4C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.1A\\Pi_data_5C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.2A\\Pi_data_5C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.3A\\Pi_data_5C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.4A\\Pi_data_5C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.5A\\Pi_data_5C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.6A\\Pi_data_5C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.7A\\Pi_data_5C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.8A\\Pi_data_5C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_0.9A\\Pi_data_5C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.0A\\Pi_data_5C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.1A\\Pi_data_5C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.2A\\Pi_data_5C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.3A\\Pi_data_5C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.4A\\Pi_data_5C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.5A\\Pi_data_5C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.6A\\Pi_data_5C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.7A\\Pi_data_5C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.8A\\Pi_data_5C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_1.9A\\Pi_data_5C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\5C_2.0A\\Pi_data_5C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.1A\\Pi_data_6C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.2A\\Pi_data_6C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.3A\\Pi_data_6C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.4A\\Pi_data_6C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.5A\\Pi_data_6C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.6A\\Pi_data_6C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.7A\\Pi_data_6C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.8A\\Pi_data_6C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_0.9A\\Pi_data_6C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.0A\\Pi_data_6C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.1A\\Pi_data_6C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.2A\\Pi_data_6C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.3A\\Pi_data_6C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.4A\\Pi_data_6C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.5A\\Pi_data_6C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.6A\\Pi_data_6C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.7A\\Pi_data_6C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.8A\\Pi_data_6C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_1.9A\\Pi_data_6C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\6C_2.0A\\Pi_data_6C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.1A\\Pi_data_7C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.2A\\Pi_data_7C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.3A\\Pi_data_7C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.4A\\Pi_data_7C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.5A\\Pi_data_7C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.6A\\Pi_data_7C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.7A\\Pi_data_7C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.8A\\Pi_data_7C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_0.9A\\Pi_data_7C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.0A\\Pi_data_7C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.1A\\Pi_data_7C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.2A\\Pi_data_7C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.3A\\Pi_data_7C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.4A\\Pi_data_7C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.5A\\Pi_data_7C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.6A\\Pi_data_7C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.7A\\Pi_data_7C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.8A\\Pi_data_7C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_1.9A\\Pi_data_7C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\7C_2.0A\\Pi_data_7C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.1A\\Pi_data_8C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.2A\\Pi_data_8C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.3A\\Pi_data_8C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.4A\\Pi_data_8C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.5A\\Pi_data_8C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.6A\\Pi_data_8C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.7A\\Pi_data_8C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.8A\\Pi_data_8C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_0.9A\\Pi_data_8C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.0A\\Pi_data_8C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.1A\\Pi_data_8C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.2A\\Pi_data_8C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.3A\\Pi_data_8C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.4A\\Pi_data_8C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.5A\\Pi_data_8C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.6A\\Pi_data_8C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.7A\\Pi_data_8C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.8A\\Pi_data_8C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_1.9A\\Pi_data_8C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\8C_2.0A\\Pi_data_8C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.1A\\Pi_data_9C_0.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.2A\\Pi_data_9C_0.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.3A\\Pi_data_9C_0.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.4A\\Pi_data_9C_0.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.5A\\Pi_data_9C_0.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.6A\\Pi_data_9C_0.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.7A\\Pi_data_9C_0.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.8A\\Pi_data_9C_0.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_0.9A\\Pi_data_9C_0.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.0A\\Pi_data_9C_1.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.1A\\Pi_data_9C_1.1A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.2A\\Pi_data_9C_1.2A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.3A\\Pi_data_9C_1.3A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.4A\\Pi_data_9C_1.4A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.5A\\Pi_data_9C_1.5A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.6A\\Pi_data_9C_1.6A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.7A\\Pi_data_9C_1.7A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.8A\\Pi_data_9C_1.8A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_1.9A\\Pi_data_9C_1.9A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\9C_2.0A\\Pi_data_9C_2.0A.npy\n",
      "Loading data from: C:\\Users\\A_structure\\bmcs_training_data\\HHLL_4C_1A_20S\\Pi_data_HHLL_4C_1A_20S.npy\n"
     ]
    }
   ],
   "source": [
    "# construc the dataset from the training data and normalize the data\n",
    "\n",
    "data_root_dir = Path(os.path.expanduser(\"~\")) / \"bmcs_training_data\"\n",
    "dataset = ViscoelasticDataset(data_root_dir, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shapes: X_data = (406000, 4), y_data = (406000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print the dataset shape and the number of samples\n",
    "print(f\"Final dataset shapes: X_data = {dataset.X_data.shape}, y_data = {dataset.y_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss: 0.025852827355265617\n",
      "Epoch 1, Batch 2, Loss: 0.8574752807617188\n",
      "Epoch 1, Batch 3, Loss: 6.379776477813721\n",
      "Epoch 1, Batch 4, Loss: 13.562705993652344\n",
      "Epoch 1, Batch 5, Loss: 0.7188438177108765\n",
      "Epoch 1, Batch 6, Loss: 0.03137912601232529\n",
      "Epoch 1, Batch 7, Loss: 0.030125737190246582\n",
      "Epoch 1, Batch 8, Loss: 0.057344261556863785\n",
      "Epoch 1, Batch 9, Loss: 4.961685657501221\n",
      "Epoch 1, Batch 10, Loss: 3.3943676948547363\n",
      "Epoch 1, Batch 11, Loss: 0.7054250240325928\n",
      "Epoch 1, Batch 12, Loss: 0.06083350628614426\n",
      "Epoch 1, Batch 13, Loss: 0.11142055690288544\n",
      "Epoch 1, Batch 14, Loss: 0.19965548813343048\n",
      "Epoch 1, Batch 15, Loss: 0.2735055088996887\n",
      "Epoch 1, Batch 16, Loss: 0.20287957787513733\n",
      "Epoch 1, Batch 17, Loss: 0.09909385442733765\n",
      "Epoch 1, Batch 18, Loss: 0.050748586654663086\n",
      "Epoch 1, Batch 19, Loss: 0.02511264570057392\n",
      "Epoch 1, Batch 20, Loss: 0.04311714321374893\n",
      "Epoch 1, Batch 21, Loss: 0.06782621890306473\n",
      "Epoch 1, Batch 22, Loss: 0.03532158583402634\n",
      "Epoch 1, Batch 23, Loss: 2.985489845275879\n",
      "Epoch 1, Batch 24, Loss: 0.02184043638408184\n",
      "Epoch 1, Batch 25, Loss: 0.07607930898666382\n",
      "Epoch 1, Batch 26, Loss: 0.08415340632200241\n",
      "Epoch 1, Batch 27, Loss: 0.10110089182853699\n",
      "Epoch 1, Batch 28, Loss: 0.08549855649471283\n",
      "Epoch 1, Batch 29, Loss: 0.376176118850708\n",
      "Epoch 1, Batch 30, Loss: 0.03870295733213425\n",
      "Epoch 1, Batch 31, Loss: 0.02761557325720787\n",
      "Epoch 1, Batch 32, Loss: 0.043644461780786514\n",
      "Epoch 1, Batch 33, Loss: 0.03545939177274704\n",
      "Epoch 1, Batch 34, Loss: 1.1136407852172852\n",
      "Epoch 1, Batch 35, Loss: 0.0238276869058609\n",
      "Epoch 1, Batch 36, Loss: 0.06226290017366409\n",
      "Epoch 1, Batch 37, Loss: 14.767708778381348\n",
      "Epoch 1, Batch 38, Loss: 0.06774502992630005\n",
      "Epoch 1, Batch 39, Loss: 0.08546832948923111\n",
      "Epoch 1, Batch 40, Loss: 0.10827748477458954\n",
      "Epoch 1, Batch 41, Loss: 0.10626371204853058\n",
      "Epoch 1, Batch 42, Loss: 0.03213784843683243\n",
      "Epoch 1, Batch 43, Loss: 0.045964375138282776\n",
      "Epoch 1, Batch 44, Loss: 0.25194141268730164\n",
      "Epoch 1, Batch 45, Loss: 0.07678350806236267\n",
      "Epoch 1, Batch 46, Loss: 0.05824341997504234\n",
      "Epoch 1, Batch 47, Loss: 0.03226206824183464\n",
      "Epoch 1, Batch 48, Loss: 0.13246183097362518\n",
      "Epoch 1, Batch 49, Loss: 0.03502617031335831\n",
      "Epoch 1, Batch 50, Loss: 0.013489419594407082\n",
      "Epoch 1, Batch 51, Loss: 0.05191213637590408\n",
      "Epoch 1, Batch 52, Loss: 0.014560435898602009\n",
      "Epoch 1, Batch 53, Loss: 0.011892504058778286\n",
      "Epoch 1, Batch 54, Loss: 0.023116253316402435\n",
      "Epoch 1, Batch 55, Loss: 0.013584407046437263\n",
      "Epoch 1, Batch 56, Loss: 0.016991430893540382\n",
      "Epoch 1, Batch 57, Loss: 0.028139963746070862\n",
      "Epoch 1, Batch 58, Loss: 0.020358717069029808\n",
      "Epoch 1, Batch 59, Loss: 0.03515486791729927\n",
      "Epoch 1, Batch 60, Loss: 0.019146937876939774\n",
      "Epoch 1, Batch 61, Loss: 0.016897452995181084\n",
      "Epoch 1, Batch 62, Loss: 0.018826773390173912\n",
      "Epoch 1, Batch 63, Loss: 0.02076977863907814\n",
      "Epoch 1, Batch 64, Loss: 0.01226003561168909\n",
      "Epoch 1, Batch 65, Loss: 0.010959647595882416\n",
      "Epoch 1, Batch 66, Loss: 1.1939445734024048\n",
      "Epoch 1, Batch 67, Loss: 0.0238373763859272\n",
      "Epoch 1, Batch 68, Loss: 0.040411561727523804\n",
      "Epoch 1, Batch 69, Loss: 0.04376714304089546\n",
      "Epoch 1, Batch 70, Loss: 0.012494247406721115\n",
      "Epoch 1, Batch 71, Loss: 0.029701832681894302\n",
      "Epoch 1, Batch 72, Loss: 0.00928356871008873\n",
      "Epoch 1, Batch 73, Loss: 0.022199461236596107\n",
      "Epoch 1, Batch 74, Loss: 0.032018158584833145\n",
      "Epoch 1, Batch 75, Loss: 0.021627752110362053\n",
      "Epoch 1, Batch 76, Loss: 0.025714807212352753\n",
      "Epoch 1, Batch 77, Loss: 0.0095241479575634\n",
      "Epoch 1, Batch 78, Loss: 2.707650899887085\n",
      "Epoch 1, Batch 79, Loss: 0.021150747314095497\n",
      "Epoch 1, Batch 80, Loss: 0.028813965618610382\n",
      "Epoch 1, Batch 81, Loss: 0.04173130542039871\n",
      "Epoch 1, Batch 82, Loss: 0.03298255056142807\n",
      "Epoch 1, Batch 83, Loss: 0.03647610917687416\n",
      "Epoch 1, Batch 84, Loss: 0.06673258543014526\n",
      "Epoch 1, Batch 85, Loss: 154.256103515625\n",
      "Epoch 1, Batch 86, Loss: 0.0892476812005043\n",
      "Epoch 1, Batch 87, Loss: 0.16792888939380646\n",
      "Epoch 1, Batch 88, Loss: 0.1320853978395462\n",
      "Epoch 1, Batch 89, Loss: 0.1078169047832489\n",
      "Epoch 1, Batch 90, Loss: 0.6006905436515808\n",
      "Epoch 1, Batch 91, Loss: 0.20895633101463318\n",
      "Epoch 1, Batch 92, Loss: 0.04613646864891052\n",
      "Epoch 1, Batch 93, Loss: 0.03651535511016846\n",
      "Epoch 1, Batch 94, Loss: 0.10967009514570236\n",
      "Epoch 1, Batch 95, Loss: 0.021513674408197403\n",
      "Epoch 1, Batch 96, Loss: 1.557898759841919\n",
      "Epoch 1, Batch 97, Loss: 0.018445493653416634\n",
      "Epoch 1, Batch 98, Loss: 0.02814682573080063\n",
      "Epoch 1, Batch 99, Loss: 0.04172120243310928\n",
      "Epoch 1, Batch 100, Loss: 0.019127026200294495\n",
      "Epoch 1, Batch 101, Loss: 0.040605153888463974\n",
      "Epoch 1, Batch 102, Loss: 133.30360412597656\n",
      "Epoch 1, Batch 103, Loss: 0.04109080880880356\n",
      "Epoch 1, Batch 104, Loss: 0.028396448120474815\n",
      "Epoch 1, Batch 105, Loss: 0.025106467306613922\n",
      "Epoch 1, Batch 106, Loss: 0.08349065482616425\n",
      "Epoch 1, Batch 107, Loss: 0.08963365107774734\n",
      "Epoch 1, Batch 108, Loss: 10.821109771728516\n",
      "Epoch 1, Batch 109, Loss: 4.356545925140381\n",
      "Epoch 1, Batch 110, Loss: 0.04250388219952583\n",
      "Epoch 1, Batch 111, Loss: 0.1071297824382782\n",
      "Epoch 1, Batch 112, Loss: 0.05177891254425049\n",
      "Epoch 1, Batch 113, Loss: 0.06716993451118469\n",
      "Epoch 1, Batch 114, Loss: 0.13038204610347748\n",
      "Epoch 1, Batch 115, Loss: 0.02779395505785942\n",
      "Epoch 1, Batch 116, Loss: 0.10358696430921555\n",
      "Epoch 1, Batch 117, Loss: 0.0360599011182785\n",
      "Epoch 1, Batch 118, Loss: 0.03043670766055584\n",
      "Epoch 1, Batch 119, Loss: 0.08270271867513657\n",
      "Epoch 1, Batch 120, Loss: 0.04477032274007797\n",
      "Epoch 1, Batch 121, Loss: 0.4277317225933075\n",
      "Epoch 1, Batch 122, Loss: 0.06310044974088669\n",
      "Epoch 1, Batch 123, Loss: 0.9080013036727905\n",
      "Epoch 1, Batch 124, Loss: 0.08972171694040298\n",
      "Epoch 1, Batch 125, Loss: 0.12876838445663452\n",
      "Epoch 1, Batch 126, Loss: 4.419978141784668\n",
      "Epoch 1, Batch 127, Loss: 0.24224257469177246\n",
      "Epoch 1, Batch 128, Loss: 0.4651402533054352\n",
      "Epoch 1, Batch 129, Loss: 0.30634555220603943\n",
      "Epoch 1, Batch 130, Loss: 0.14960774779319763\n",
      "Epoch 1, Batch 131, Loss: 0.34005069732666016\n",
      "Epoch 1, Batch 132, Loss: 0.21437692642211914\n",
      "Epoch 1, Batch 133, Loss: 0.019484827294945717\n",
      "Epoch 1, Batch 134, Loss: 0.2433059811592102\n",
      "Epoch 1, Batch 135, Loss: 0.18154184520244598\n",
      "Epoch 1, Batch 136, Loss: 0.8519687056541443\n",
      "Epoch 1, Batch 137, Loss: 0.1393413543701172\n",
      "Epoch 1, Batch 138, Loss: 0.11773239076137543\n",
      "Epoch 1, Batch 139, Loss: 2.151620388031006\n",
      "Epoch 1, Batch 140, Loss: 0.0496075265109539\n",
      "Epoch 1, Batch 141, Loss: 0.055636778473854065\n",
      "Epoch 1, Batch 142, Loss: 0.03459535911679268\n",
      "Epoch 1, Batch 143, Loss: 0.08076091855764389\n",
      "Epoch 1, Batch 144, Loss: 0.037451062351465225\n",
      "Epoch 1, Batch 145, Loss: 0.018210459500551224\n",
      "Epoch 1, Batch 146, Loss: 0.041780710220336914\n",
      "Epoch 1, Batch 147, Loss: 0.021002523601055145\n",
      "Epoch 1, Batch 148, Loss: 0.016231317073106766\n",
      "Epoch 1, Batch 149, Loss: 0.027185838669538498\n",
      "Epoch 1, Batch 150, Loss: 0.05557345598936081\n",
      "Epoch 1, Batch 151, Loss: 0.0520344153046608\n",
      "Epoch 1, Batch 152, Loss: 0.03663662075996399\n",
      "Epoch 1, Batch 153, Loss: 0.04515433683991432\n",
      "Epoch 1, Batch 154, Loss: 0.07937905192375183\n",
      "Epoch 1, Batch 155, Loss: 0.019041767343878746\n",
      "Epoch 1, Batch 156, Loss: 0.0857362374663353\n",
      "Epoch 1, Batch 157, Loss: 0.0380096398293972\n",
      "Epoch 1, Batch 158, Loss: 0.021448584273457527\n",
      "Epoch 1, Batch 159, Loss: 0.05076981708407402\n",
      "Epoch 1, Batch 160, Loss: 0.06564148515462875\n",
      "Epoch 1, Batch 161, Loss: 0.018924027681350708\n",
      "Epoch 1, Batch 162, Loss: 0.046540118753910065\n",
      "Epoch 1, Batch 163, Loss: 1.6390833854675293\n",
      "Epoch 1, Batch 164, Loss: 0.0867525115609169\n",
      "Epoch 1, Batch 165, Loss: 0.03194519504904747\n",
      "Epoch 1, Batch 166, Loss: 0.028115831315517426\n",
      "Epoch 1, Batch 167, Loss: 0.08603398501873016\n",
      "Epoch 1, Batch 168, Loss: 0.07427186518907547\n",
      "Epoch 1, Batch 169, Loss: 0.013941049575805664\n",
      "Epoch 1, Batch 170, Loss: 4.588132381439209\n",
      "Epoch 1, Batch 171, Loss: 0.0972389206290245\n",
      "Epoch 1, Batch 172, Loss: 0.044527482241392136\n",
      "Epoch 1, Batch 173, Loss: 0.07884490489959717\n",
      "Epoch 1, Batch 174, Loss: 0.020329846069216728\n",
      "Epoch 1, Batch 175, Loss: 0.024519478902220726\n",
      "Epoch 1, Batch 176, Loss: 0.020756343379616737\n",
      "Epoch 1, Batch 177, Loss: 0.05571199953556061\n",
      "Epoch 1, Batch 178, Loss: 0.03736881911754608\n",
      "Epoch 1, Batch 179, Loss: 0.016023317351937294\n",
      "Epoch 1, Batch 180, Loss: 0.014688209630548954\n",
      "Epoch 1, Batch 181, Loss: 0.017346294596791267\n",
      "Epoch 1, Batch 182, Loss: 0.02336912602186203\n",
      "Epoch 1, Batch 183, Loss: 0.9244081377983093\n",
      "Epoch 1, Batch 184, Loss: 0.5379948019981384\n",
      "Epoch 1, Batch 185, Loss: 0.012774764560163021\n",
      "Epoch 1, Batch 186, Loss: 0.010679968632757664\n",
      "Epoch 1, Batch 187, Loss: 0.013901844620704651\n",
      "Epoch 1, Batch 188, Loss: 0.010153800249099731\n",
      "Epoch 1, Batch 189, Loss: 0.009440328925848007\n",
      "Epoch 1, Batch 190, Loss: 0.010612662881612778\n",
      "Epoch 1, Batch 191, Loss: 0.01147700846195221\n",
      "Epoch 1, Batch 192, Loss: 0.006758981849998236\n",
      "Epoch 1, Batch 193, Loss: 0.008235268294811249\n",
      "Epoch 1, Batch 194, Loss: 0.010635516606271267\n",
      "Epoch 1, Batch 195, Loss: 0.009845398366451263\n",
      "Epoch 1, Batch 196, Loss: 1.2926533222198486\n",
      "Epoch 1, Batch 197, Loss: 0.01162464264780283\n",
      "Epoch 1, Batch 198, Loss: 0.0060683428309857845\n",
      "Epoch 1, Batch 199, Loss: 0.00763342808932066\n",
      "Epoch 1, Batch 200, Loss: 0.4596351683139801\n",
      "Epoch 1, Batch 201, Loss: 0.010116354562342167\n",
      "Epoch 1, Batch 202, Loss: 0.009846551343798637\n",
      "Epoch 1, Batch 203, Loss: 0.011173688806593418\n",
      "Epoch 1, Batch 204, Loss: 0.00999112892895937\n",
      "Epoch 1, Batch 205, Loss: 0.008961587212979794\n",
      "Epoch 1, Batch 206, Loss: 0.003716652514412999\n",
      "Epoch 1, Batch 207, Loss: 0.01044574286788702\n",
      "Epoch 1, Batch 208, Loss: 0.010919472202658653\n",
      "Epoch 1, Batch 209, Loss: 0.010029244236648083\n",
      "Epoch 1, Batch 210, Loss: 0.00934572797268629\n",
      "Epoch 1, Batch 211, Loss: 0.015548850409686565\n",
      "Epoch 1, Batch 212, Loss: 0.0060030012391507626\n",
      "Epoch 1, Batch 213, Loss: 0.007426951080560684\n",
      "Epoch 1, Batch 214, Loss: 0.008023174479603767\n",
      "Epoch 1, Batch 215, Loss: 0.0032049540895968676\n",
      "Epoch 1, Batch 216, Loss: 0.007200941909104586\n",
      "Epoch 1, Batch 217, Loss: 0.009480537846684456\n",
      "Epoch 1, Batch 218, Loss: 0.00396523205563426\n",
      "Epoch 1, Batch 219, Loss: 0.003982966765761375\n",
      "Epoch 1, Batch 220, Loss: 0.00985153391957283\n",
      "Epoch 1, Batch 221, Loss: 0.00496408436447382\n",
      "Epoch 1, Batch 222, Loss: 0.005148769356310368\n",
      "Epoch 1, Batch 223, Loss: 0.004113840404897928\n",
      "Epoch 1, Batch 224, Loss: 0.00500181969255209\n",
      "Epoch 1, Batch 225, Loss: 0.009412438608705997\n",
      "Epoch 1, Batch 226, Loss: 0.006868109107017517\n",
      "Epoch 1, Batch 227, Loss: 0.0034157978370785713\n",
      "Epoch 1, Batch 228, Loss: 0.03070877678692341\n",
      "Epoch 1, Batch 229, Loss: 0.003516628174111247\n",
      "Epoch 1, Batch 230, Loss: 1.0577951669692993\n",
      "Epoch 1, Batch 231, Loss: 0.010988401249051094\n",
      "Epoch 1, Batch 232, Loss: 0.005528926849365234\n",
      "Epoch 1, Batch 233, Loss: 0.003714106511324644\n",
      "Epoch 1, Batch 234, Loss: 0.00833162758499384\n",
      "Epoch 1, Batch 235, Loss: 0.008720170706510544\n",
      "Epoch 1, Batch 236, Loss: 0.00886992271989584\n",
      "Epoch 1, Batch 237, Loss: 0.0029519025702029467\n",
      "Epoch 1, Batch 238, Loss: 0.00933777168393135\n",
      "Epoch 1, Batch 239, Loss: 0.007592480629682541\n",
      "Epoch 1, Batch 240, Loss: 0.003969893325120211\n",
      "Epoch 1, Batch 241, Loss: 0.013278113678097725\n",
      "Epoch 1, Batch 242, Loss: 0.3665696084499359\n",
      "Epoch 1, Batch 243, Loss: 0.0030859634280204773\n",
      "Epoch 1, Batch 244, Loss: 0.00450802780687809\n",
      "Epoch 1, Batch 245, Loss: 0.012171467766165733\n",
      "Epoch 1, Batch 246, Loss: 0.006949943024665117\n",
      "Epoch 1, Batch 247, Loss: 0.01252017728984356\n",
      "Epoch 1, Batch 248, Loss: 0.002577481558546424\n",
      "Epoch 1, Batch 249, Loss: 0.004735125228762627\n",
      "Epoch 1, Batch 250, Loss: 0.005224928725510836\n",
      "Epoch 1, Batch 251, Loss: 0.0046689012087881565\n",
      "Epoch 1, Batch 252, Loss: 0.007665189448744059\n",
      "Epoch 1, Batch 253, Loss: 3.489760160446167\n",
      "Epoch 1, Batch 254, Loss: 0.004477666690945625\n",
      "Epoch 1, Batch 255, Loss: 0.005265953950583935\n",
      "Epoch 1, Batch 256, Loss: 0.007842984981834888\n",
      "Epoch 1, Batch 257, Loss: 0.006509315222501755\n",
      "Epoch 1, Batch 258, Loss: 0.0028903258498758078\n",
      "Epoch 1, Batch 259, Loss: 0.004623063374310732\n",
      "Epoch 1, Batch 260, Loss: 0.002666925312951207\n",
      "Epoch 1, Batch 261, Loss: 0.004492094740271568\n",
      "Epoch 1, Batch 262, Loss: 0.004140052944421768\n",
      "Epoch 1, Batch 263, Loss: 0.004079272970557213\n",
      "Epoch 1, Batch 264, Loss: 0.004212404601275921\n",
      "Epoch 1, Batch 265, Loss: 0.0032925065606832504\n",
      "Epoch 1, Batch 266, Loss: 0.0015724054537713528\n",
      "Epoch 1, Batch 267, Loss: 0.004859718959778547\n",
      "Epoch 1, Batch 268, Loss: 0.005750469863414764\n",
      "Epoch 1, Batch 269, Loss: 0.0027063661254942417\n",
      "Epoch 1, Batch 270, Loss: 0.0038267343770712614\n",
      "Epoch 1, Batch 271, Loss: 0.008523980155587196\n",
      "Epoch 1, Batch 272, Loss: 0.007354654371738434\n",
      "Epoch 1, Batch 273, Loss: 0.009801344014704227\n",
      "Epoch 1, Batch 274, Loss: 0.008145089261233807\n",
      "Epoch 1, Batch 275, Loss: 0.003429062431678176\n",
      "Epoch 1, Batch 276, Loss: 0.005906358826905489\n",
      "Epoch 1, Batch 277, Loss: 0.005969214718788862\n",
      "Epoch 1, Batch 278, Loss: 0.00799410417675972\n",
      "Epoch 1, Batch 279, Loss: 0.004338872618973255\n",
      "Epoch 1, Batch 280, Loss: 0.0023100401740521193\n",
      "Epoch 1, Batch 281, Loss: 0.00937634613364935\n",
      "Epoch 1, Batch 282, Loss: 0.0022630570456385612\n",
      "Epoch 1, Batch 283, Loss: 0.07243195176124573\n",
      "Epoch 1, Batch 284, Loss: 0.011282422579824924\n",
      "Epoch 1, Batch 285, Loss: 0.012973272241652012\n",
      "Epoch 1, Batch 286, Loss: 0.0036342917010188103\n",
      "Epoch 1, Batch 287, Loss: 0.06244382634758949\n",
      "Epoch 1, Batch 288, Loss: 0.010516831651329994\n",
      "Epoch 1, Batch 289, Loss: 0.011639142408967018\n",
      "Epoch 1, Batch 290, Loss: 0.014694459736347198\n",
      "Epoch 1, Batch 291, Loss: 0.7962023615837097\n",
      "Epoch 1, Batch 292, Loss: 0.0062169041484594345\n",
      "Epoch 1, Batch 293, Loss: 0.01852225512266159\n",
      "Epoch 1, Batch 294, Loss: 0.18704044818878174\n",
      "Epoch 1, Batch 295, Loss: 0.0058234455063939095\n",
      "Epoch 1, Batch 296, Loss: 0.010271193459630013\n",
      "Epoch 1, Batch 297, Loss: 0.6047495007514954\n",
      "Epoch 1, Batch 298, Loss: 0.015916191041469574\n",
      "Epoch 1, Batch 299, Loss: 0.029480116441845894\n",
      "Epoch 1, Batch 300, Loss: 0.03448028489947319\n",
      "Epoch 1, Batch 301, Loss: 0.028259197250008583\n",
      "Epoch 1, Batch 302, Loss: 0.004838917404413223\n",
      "Epoch 1, Batch 303, Loss: 0.018283644691109657\n",
      "Epoch 1, Batch 304, Loss: 0.007033706642687321\n",
      "Epoch 1, Batch 305, Loss: 0.015436658635735512\n",
      "Epoch 1, Batch 306, Loss: 0.024732813239097595\n",
      "Epoch 1, Batch 307, Loss: 0.01387183740735054\n",
      "Epoch 1, Batch 308, Loss: 0.030212167650461197\n",
      "Epoch 1, Batch 309, Loss: 0.005421818699687719\n",
      "Epoch 1, Batch 310, Loss: 0.008401108905673027\n",
      "Epoch 1, Batch 311, Loss: 0.018442228436470032\n",
      "Epoch 1, Batch 312, Loss: 0.006697856821119785\n",
      "Epoch 1, Batch 313, Loss: 0.003331755753606558\n",
      "Epoch 1, Batch 314, Loss: 0.010133378207683563\n",
      "Epoch 1, Batch 315, Loss: 0.03442883491516113\n",
      "Epoch 1, Batch 316, Loss: 0.22345232963562012\n",
      "Epoch 1, Batch 317, Loss: 0.008083846420049667\n",
      "Epoch 1, Batch 318, Loss: 0.01884976588189602\n",
      "Epoch 1, Batch 319, Loss: 0.0035852245055139065\n",
      "Epoch 1, Batch 320, Loss: 0.002136100083589554\n",
      "Epoch 1, Batch 321, Loss: 0.0027715095784515142\n",
      "Epoch 1, Batch 322, Loss: 1.613011360168457\n",
      "Epoch 1, Batch 323, Loss: 0.006127260625362396\n",
      "Epoch 1, Batch 324, Loss: 0.08284903317689896\n",
      "Epoch 1, Batch 325, Loss: 0.006474787835031748\n",
      "Epoch 1, Batch 326, Loss: 0.013197005726397038\n",
      "Epoch 1, Batch 327, Loss: 0.018063828349113464\n",
      "Epoch 1, Batch 328, Loss: 0.0045321062207221985\n",
      "Epoch 1, Batch 329, Loss: 0.011839748360216618\n",
      "Epoch 1, Batch 330, Loss: 0.023691724985837936\n",
      "Epoch 1, Batch 331, Loss: 0.018483638763427734\n",
      "Epoch 1, Batch 332, Loss: 0.009066276252269745\n",
      "Epoch 1, Batch 333, Loss: 0.007188689894974232\n",
      "Epoch 1, Batch 334, Loss: 0.011171677149832249\n",
      "Epoch 1, Batch 335, Loss: 0.024328874424099922\n",
      "Epoch 1, Batch 336, Loss: 0.003934058360755444\n",
      "Epoch 1, Batch 337, Loss: 3.2191877365112305\n",
      "Epoch 1, Batch 338, Loss: 0.2880658209323883\n",
      "Epoch 1, Batch 339, Loss: 0.00893778819590807\n",
      "Epoch 1, Batch 340, Loss: 0.01774272695183754\n",
      "Epoch 1, Batch 341, Loss: 0.047117929905653\n",
      "Epoch 1, Batch 342, Loss: 0.009845737367868423\n",
      "Epoch 1, Batch 343, Loss: 0.05204636603593826\n",
      "Epoch 1, Batch 344, Loss: 0.035011496394872665\n",
      "Epoch 1, Batch 345, Loss: 0.01067593228071928\n",
      "Epoch 1, Batch 346, Loss: 0.01376068964600563\n",
      "Epoch 1, Batch 347, Loss: 0.03935972601175308\n",
      "Epoch 1, Batch 348, Loss: 0.09744793176651001\n",
      "Epoch 1, Batch 349, Loss: 0.01012546569108963\n",
      "Epoch 1, Batch 350, Loss: 0.03196152672171593\n",
      "Epoch 1, Batch 351, Loss: 0.03998887538909912\n",
      "Epoch 1, Batch 352, Loss: 0.023421548306941986\n",
      "Epoch 1, Batch 353, Loss: 0.008908350951969624\n",
      "Epoch 1, Batch 354, Loss: 0.01589411310851574\n",
      "Epoch 1, Batch 355, Loss: 0.026903914287686348\n",
      "Epoch 1, Batch 356, Loss: 0.02606220170855522\n",
      "Epoch 1, Batch 357, Loss: 0.012889638543128967\n",
      "Epoch 1, Batch 358, Loss: 0.011308358050882816\n",
      "Epoch 1, Batch 359, Loss: 0.011254170909523964\n",
      "Epoch 1, Batch 360, Loss: 0.013226203620433807\n",
      "Epoch 1, Batch 361, Loss: 0.012020047754049301\n",
      "Epoch 1, Batch 362, Loss: 0.002755729714408517\n",
      "Epoch 1, Batch 363, Loss: 0.007338954135775566\n",
      "Epoch 1, Batch 364, Loss: 0.009657438844442368\n",
      "Epoch 1, Batch 365, Loss: 0.008021648973226547\n",
      "Epoch 1, Batch 366, Loss: 0.004552856087684631\n",
      "Epoch 1, Batch 367, Loss: 0.006408650428056717\n",
      "Epoch 1, Batch 368, Loss: 0.009402278810739517\n",
      "Epoch 1, Batch 369, Loss: 0.009223478846251965\n",
      "Epoch 1, Batch 370, Loss: 0.005033654626458883\n",
      "Epoch 1, Batch 371, Loss: 0.008267106488347054\n",
      "Epoch 1, Batch 372, Loss: 0.006997588090598583\n",
      "Epoch 1, Batch 373, Loss: 0.04317691549658775\n",
      "Epoch 1, Batch 374, Loss: 0.004343681503087282\n",
      "Epoch 1, Batch 375, Loss: 0.0031522747594863176\n",
      "Epoch 1, Batch 376, Loss: 0.004478588234633207\n",
      "Epoch 1, Batch 377, Loss: 0.004393620416522026\n",
      "Epoch 1, Batch 378, Loss: 0.0024418733082711697\n",
      "Epoch 1, Batch 379, Loss: 0.0042976923286914825\n",
      "Epoch 1, Batch 380, Loss: 0.8273939490318298\n",
      "Epoch 1, Batch 381, Loss: 0.004538861569017172\n",
      "Epoch 1, Batch 382, Loss: 0.004205516539514065\n",
      "Epoch 1, Batch 383, Loss: 0.0033429621253162622\n",
      "Epoch 1, Batch 384, Loss: 0.0032979361712932587\n",
      "Epoch 1, Batch 385, Loss: 0.002387647284194827\n",
      "Epoch 1, Batch 386, Loss: 0.0024757017381489277\n",
      "Epoch 1, Batch 387, Loss: 0.0018791940528899431\n",
      "Epoch 1, Batch 388, Loss: 0.0026369853876531124\n",
      "Epoch 1, Batch 389, Loss: 0.004108186345547438\n",
      "Epoch 1, Batch 390, Loss: 0.0015969030791893601\n",
      "Epoch 1, Batch 391, Loss: 0.0036303168162703514\n",
      "Epoch 1, Batch 392, Loss: 0.003104354254901409\n",
      "Epoch 1, Batch 393, Loss: 0.002849690616130829\n",
      "Epoch 1, Batch 394, Loss: 0.0035848645493388176\n",
      "Epoch 1, Batch 395, Loss: 0.0983283519744873\n",
      "Epoch 1, Batch 396, Loss: 0.003974132239818573\n",
      "Epoch 1, Batch 397, Loss: 0.012377540580928326\n",
      "Epoch 1, Batch 398, Loss: 0.09816869348287582\n",
      "Epoch 1, Batch 399, Loss: 0.010024646297097206\n",
      "Epoch 1, Batch 400, Loss: 0.005827787797898054\n",
      "Epoch 1, Batch 401, Loss: 0.012348152697086334\n",
      "Epoch 1, Batch 402, Loss: 0.007708422373980284\n",
      "Epoch 1, Batch 403, Loss: 0.0034992906730622053\n",
      "Epoch 1, Batch 404, Loss: 0.00478495704010129\n",
      "Epoch 1, Batch 405, Loss: 0.005682831164449453\n",
      "Epoch 1, Batch 406, Loss: 0.025389691814780235\n",
      "Epoch 1, Batch 407, Loss: 0.011791606433689594\n",
      "Epoch 1, Batch 408, Loss: 0.005918396171182394\n",
      "Epoch 1, Batch 409, Loss: 0.007440080866217613\n",
      "Epoch 1, Batch 410, Loss: 0.007467141840606928\n",
      "Epoch 1, Batch 411, Loss: 2.0799448490142822\n",
      "Epoch 1, Batch 412, Loss: 0.005919374991208315\n",
      "Epoch 1, Batch 413, Loss: 0.0056510441936552525\n",
      "Epoch 1, Batch 414, Loss: 0.006781787611544132\n",
      "Epoch 1, Batch 415, Loss: 0.002596186939626932\n",
      "Epoch 1, Batch 416, Loss: 0.002286662347614765\n",
      "Epoch 1, Batch 417, Loss: 0.005286247935146093\n",
      "Epoch 1, Batch 418, Loss: 0.007823524996638298\n",
      "Epoch 1, Batch 419, Loss: 0.011602567508816719\n",
      "Epoch 1, Batch 420, Loss: 0.002560059539973736\n",
      "Epoch 1, Batch 421, Loss: 0.005629442632198334\n",
      "Epoch 1, Batch 422, Loss: 0.007469420321285725\n",
      "Epoch 1, Batch 423, Loss: 0.004707422573119402\n",
      "Epoch 1, Batch 424, Loss: 0.0017047352157533169\n",
      "Epoch 1, Batch 425, Loss: 0.0043935212306678295\n",
      "Epoch 1, Batch 426, Loss: 0.0031712378840893507\n",
      "Epoch 1, Batch 427, Loss: 0.005948404315859079\n",
      "Epoch 1, Batch 428, Loss: 0.00298773474059999\n",
      "Epoch 1, Batch 429, Loss: 0.005160119384527206\n",
      "Epoch 1, Batch 430, Loss: 0.5848672986030579\n",
      "Epoch 1, Batch 431, Loss: 0.15459521114826202\n",
      "Epoch 1, Batch 432, Loss: 0.0018822640413418412\n",
      "Epoch 1, Batch 433, Loss: 0.004471279680728912\n",
      "Epoch 1, Batch 434, Loss: 0.001720065250992775\n",
      "Epoch 1, Batch 435, Loss: 0.0022248406894505024\n",
      "Epoch 1, Batch 436, Loss: 0.010088866576552391\n",
      "Epoch 1, Batch 437, Loss: 0.0012790677137672901\n",
      "Epoch 1, Batch 438, Loss: 0.0025718742981553078\n",
      "Epoch 1, Batch 439, Loss: 0.0027136742137372494\n",
      "Epoch 1, Batch 440, Loss: 0.0013089041458442807\n",
      "Epoch 1, Batch 441, Loss: 0.0023558768443763256\n",
      "Epoch 1, Batch 442, Loss: 0.002689011860638857\n",
      "Epoch 1, Batch 443, Loss: 0.0015060582663863897\n",
      "Epoch 1, Batch 444, Loss: 0.0018322746036574244\n",
      "Epoch 1, Batch 445, Loss: 0.002793250372633338\n",
      "Epoch 1, Batch 446, Loss: 0.004135345574468374\n",
      "Epoch 1, Batch 447, Loss: 0.005351664964109659\n",
      "Epoch 1, Batch 448, Loss: 0.002328498288989067\n",
      "Epoch 1, Batch 449, Loss: 0.0009629407431930304\n",
      "Epoch 1, Batch 450, Loss: 0.0018622471252456307\n",
      "Epoch 1, Batch 451, Loss: 0.027334246784448624\n",
      "Epoch 1, Batch 452, Loss: 0.0012986140791326761\n",
      "Epoch 1, Batch 453, Loss: 0.004166051745414734\n",
      "Epoch 1, Batch 454, Loss: 0.0027235299348831177\n",
      "Epoch 1, Batch 455, Loss: 0.003981731832027435\n",
      "Epoch 1, Batch 456, Loss: 0.00243767979554832\n",
      "Epoch 1, Batch 457, Loss: 0.0017252665711566806\n",
      "Epoch 1, Batch 458, Loss: 0.002215226413682103\n",
      "Epoch 1, Batch 459, Loss: 0.002700098557397723\n",
      "Epoch 1, Batch 460, Loss: 0.0025331787765026093\n",
      "Epoch 1, Batch 461, Loss: 0.0034036883153021336\n",
      "Epoch 1, Batch 462, Loss: 0.00294927298091352\n",
      "Epoch 1, Batch 463, Loss: 0.0025358800776302814\n",
      "Epoch 1, Batch 464, Loss: 0.0033741029910743237\n",
      "Epoch 1, Batch 465, Loss: 0.0022571361623704433\n",
      "Epoch 1, Batch 466, Loss: 0.002527564065530896\n",
      "Epoch 1, Batch 467, Loss: 0.0007607972365804017\n",
      "Epoch 1, Batch 468, Loss: 0.001090550096705556\n",
      "Epoch 1, Batch 469, Loss: 0.002256104489788413\n",
      "Epoch 1, Batch 470, Loss: 0.0022896951995790005\n",
      "Epoch 1, Batch 471, Loss: 0.0008505050791427493\n",
      "Epoch 1, Batch 472, Loss: 0.002437543822452426\n",
      "Epoch 1, Batch 473, Loss: 0.0021323449909687042\n",
      "Epoch 1, Batch 474, Loss: 0.0019080833299085498\n",
      "Epoch 1, Batch 475, Loss: 0.00237724045291543\n",
      "Epoch 1, Batch 476, Loss: 0.001958359032869339\n",
      "Epoch 1, Batch 477, Loss: 0.001718459534458816\n",
      "Epoch 1, Batch 478, Loss: 0.0019246606389060616\n",
      "Epoch 1, Batch 479, Loss: 0.0030499633867293596\n",
      "Epoch 1, Batch 480, Loss: 0.0023863548412919044\n",
      "Epoch 1, Batch 481, Loss: 0.002203754149377346\n",
      "Epoch 1, Batch 482, Loss: 0.002116408897563815\n",
      "Epoch 1, Batch 483, Loss: 0.0007253143703565001\n",
      "Epoch 1, Batch 484, Loss: 0.0013652053894475102\n",
      "Epoch 1, Batch 485, Loss: 0.003789858426898718\n",
      "Epoch 1, Batch 486, Loss: 0.0008185119368135929\n",
      "Epoch 1, Batch 487, Loss: 0.0015639250632375479\n",
      "Epoch 1, Batch 488, Loss: 0.004238487686961889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m VE_TimeIntegrationPredictor()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model= train_nn(dataset, epochs=100, batch_size=2000, initial_lr=0.01, lr_decay=0.99, shuffle=False, model_filename=\"ve_ivp_e100_b2000.pth\")\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mve_pi_p_e100_b32_V2.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\Coding\\bmcs_ml\\bmcs_ml\\ml_pi_predictor\\ml_pi_predictor_2.py:97\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(dataset, epochs, batch_size, initial_lr, lr_decay, shuffle, model_filename)\u001b[0m\n\u001b[0;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m     96\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 97\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    233\u001b[0m         group,\n\u001b[0;32m    234\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         state_steps,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[1;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\A_structure\\anaconda3\\envs\\bmcs_ml\\Lib\\site-packages\\torch\\optim\\adamw.py:426\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    425\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[1;32m--> 426\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    429\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VE_TimeIntegrationPredictor().to(device)\n",
    "\n",
    "# model= train_nn(dataset, epochs=100, batch_size=2000, initial_lr=0.01, lr_decay=0.99, shuffle=False, model_filename=\"ve_ivp_e100_b2000.pth\")\n",
    "\n",
    "model = train_nn(dataset, epochs=100, batch_size=1, initial_lr=0.01, lr_decay=0.99, shuffle=True, model_filename=\"ve_pi_p_e100_b32_V2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmcs_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
