{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data Generation from Part 1\n",
    "def generate_synthetic_data():\n",
    "    # Parameters for the viscoelastic model\n",
    "    A = 200  # Elastic stiffness matrix in MPa\n",
    "    Q = 10   # Kernel decay rate\n",
    "    D = 50   # Damping coefficient in MPaÂ·s\n",
    "    dt = 0.01  # Time step\n",
    "\n",
    "    total_time = 3  \n",
    "    time = np.arange(0, total_time, dt)  \n",
    "    custom_strain_points = [0, 0.05, -0.05, 0.1, -0.1, 0.15, -0.15, 0]\n",
    "    custom_time_points = np.linspace(0, total_time, len(custom_strain_points))\n",
    "\n",
    "    # Interpolate to create the strain path\n",
    "    strain = np.interp(time, custom_time_points, custom_strain_points)\n",
    "\n",
    "    # Strain rate (dE/dt) using numerical differentiation\n",
    "    strain_rate = np.gradient(strain, dt)\n",
    "\n",
    "    stress = np.zeros_like(time)\n",
    "\n",
    "    # Compute stress using the convolution integral\n",
    "    for i in range(1, len(time)):\n",
    "        elastic_stress = A * strain[i]\n",
    "        viscoelastic_stress = 0\n",
    "        for j in range(i):\n",
    "            kernel = np.exp(-Q * (time[i] - time[j]))  # Decaying kernel\n",
    "            viscoelastic_stress += kernel * D * strain_rate[j] * dt  # Convolution for viscoelastic contribution\n",
    "        stress[i] = elastic_stress + viscoelastic_stress\n",
    "\n",
    "    # Convert to torch tensors for training\n",
    "    strain_tensor = torch.tensor(strain, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)\n",
    "    stress_tensor = torch.tensor(stress, dtype=torch.float32).unsqueeze(1)  # Shape (N, 1)\n",
    "    \n",
    "    return strain_tensor, stress_tensor, time, strain, stress\n",
    "\n",
    "# Generate the data\n",
    "strain_tensor, stress_tensor, time, strain, stress = generate_synthetic_data()\n",
    "\n",
    "# Plot the strain-time curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(time, strain, label=\"Strain vs Time\", color='b', linewidth=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Strain\")\n",
    "plt.title(\"Strain-Time Curve (Custom Path in 3 Seconds)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the stress-strain curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(strain, stress, label=\"Stress vs Strain\", color='r', linewidth=2)\n",
    "plt.xlabel(\"Strain\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Stress-Strain Curve (Viscoelastic Material)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(strain_tensor, stress_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Custom SoftplusSquared activation function\n",
    "class SoftplusSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.softplus(x).pow(2)\n",
    "\n",
    "# Define the neural network for N_theta(E)\n",
    "class N_theta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(N_theta, self).__init__()\n",
    "        \n",
    "        # Define a fully connected neural network (Elastic density function)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),        # Input layer: 1 input (E), 64 output\n",
    "            SoftplusSquared(),       # Custom activation function\n",
    "            nn.Linear(64, 64),       # Hidden layer: 64 input, 64 output\n",
    "            SoftplusSquared(),       # Custom activation function\n",
    "            nn.Linear(64, 1)         # Output layer: scalar output (elastic density)\n",
    "        )\n",
    "    \n",
    "    def forward(self, E):\n",
    "        return self.fc(E)\n",
    "\n",
    "# Define the function W_theta(E) based on N_theta(E) and its derivative\n",
    "class W_theta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W_theta, self).__init__()\n",
    "        \n",
    "        # Instantiate the neural network N_theta(E)\n",
    "        self.N_theta = N_theta()\n",
    "    \n",
    "    def forward(self, E):\n",
    "        # Forward pass through the neural network N_theta(E)\n",
    "        N_E = self.N_theta(E)\n",
    "        \n",
    "        # Compute the gradient of N_theta(E) with respect to E at E = 0\n",
    "        E_zero = torch.tensor([[0.0]], requires_grad=True)  # Input at E = 0\n",
    "        N_E_zero = self.N_theta(E_zero)  # Evaluate at E = 0\n",
    "        \n",
    "        # Compute the gradient dN_theta/dE at E = 0\n",
    "        grad_N_E_zero = torch.autograd.grad(N_E_zero, E_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute W_theta(E) = N_theta(E) - grad(N_theta)(0) * E\n",
    "        W_E = N_E - grad_N_E_zero * E\n",
    "        \n",
    "        return W_E\n",
    "\n",
    "\n",
    "\n",
    "# Define the neural network M_Phi(delta)\n",
    "class M_Phi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M_Phi, self).__init__()\n",
    "        \n",
    "        # Define a fully connected neural network\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),        # Input layer: 1 input (delta), 64 output\n",
    "            nn.ReLU(),               # Activation function (convexity)\n",
    "            nn.Linear(64, 64),       # Hidden layer: 64 input, 64 output\n",
    "            nn.ReLU(),               # Activation function (convexity)\n",
    "            nn.Linear(64, 1)         # Output layer: scalar output\n",
    "        )\n",
    "    \n",
    "    def forward(self, delta):\n",
    "        return self.fc(delta)\n",
    "\n",
    "# Define the function V_Phi(E, alpha) based on M_Phi(delta) and its derivative\n",
    "class V_Phi(nn.Module):\n",
    "    def __init__(self, C1=1.0, C2=1.0):\n",
    "        super(V_Phi, self).__init__()\n",
    "        \n",
    "        # Instantiate the neural network M_Phi(delta)\n",
    "        self.M_Phi = M_Phi()\n",
    "        \n",
    "        # Constants C1 and C2\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "    \n",
    "    def forward(self, E, alpha):\n",
    "        # Compute delta = C1 * E + C2 * alpha\n",
    "        delta = self.C1 * E + self.C2 * alpha\n",
    "        \n",
    "        # Forward pass through the neural network M_Phi(delta)\n",
    "        M_delta = self.M_Phi(delta)\n",
    "        \n",
    "        # Compute the gradient of M_Phi(delta) with respect to delta at delta = 0\n",
    "        delta_zero = torch.tensor([[0.0]], requires_grad=True)  # Input at delta = 0\n",
    "        M_delta_zero = self.M_Phi(delta_zero)  # Evaluate at delta = 0\n",
    "        \n",
    "        # Compute the gradient dM_Phi/d(delta) at delta = 0\n",
    "        grad_M_delta_zero = torch.autograd.grad(M_delta_zero, delta_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute V_Phi(E, alpha) = M_Phi(delta) - grad(M_Phi)(0) * delta\n",
    "        V_E_alpha = M_delta - grad_M_delta_zero * delta\n",
    "        \n",
    "        return V_E_alpha\n",
    "\n",
    "\n",
    "# Define the neural network D_Gamma(beta, E)\n",
    "class D_Gamma(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_Gamma, self).__init__()\n",
    "        \n",
    "        # Define a fully connected neural network\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2, 64),        # Input layer: 2 inputs (beta, E), 64 output\n",
    "            nn.ReLU(),               # Activation function (convexity)\n",
    "            nn.Linear(64, 64),       # Hidden layer: 64 input, 64 output\n",
    "            nn.ReLU(),               # Activation function (convexity)\n",
    "            nn.Linear(64, 1)         # Output layer: scalar output\n",
    "        )\n",
    "    \n",
    "    def forward(self, beta, E):\n",
    "        # Concatenate beta and E as input\n",
    "        input_data = torch.cat([beta, E], dim=1)\n",
    "        return self.fc(input_data)\n",
    "\n",
    "# Define the function G_Gamma(beta, E) based on D_Gamma(beta, E) and its derivative\n",
    "class G_Gamma(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_Gamma, self).__init__()\n",
    "        \n",
    "        # Instantiate the neural network D_Gamma(beta, E)\n",
    "        self.D_Gamma = D_Gamma()\n",
    "    \n",
    "    def forward(self, beta, E):\n",
    "        # Forward pass through the neural network D_Gamma(beta, E)\n",
    "        D_beta_E = self.D_Gamma(beta, E)\n",
    "        \n",
    "        # Compute the gradient of D_Gamma(beta, E) with respect to beta at beta = 0\n",
    "        beta_zero = torch.zeros_like(beta, requires_grad=True)  # Input at beta = 0\n",
    "        D_beta_zero_E = self.D_Gamma(beta_zero, E)  # Evaluate at beta = 0\n",
    "        \n",
    "        # Compute the gradient dD_Gamma/d(beta) at beta = 0\n",
    "        grad_D_beta_zero = torch.autograd.grad(D_beta_zero_E, beta_zero, create_graph=True)[0]\n",
    "\n",
    "        # Compute G_Gamma(beta, E) = D_Gamma(beta, E) - grad(D_Gamma)(0, E) * beta\n",
    "        G_beta_E = D_beta_E - grad_D_beta_zero * beta\n",
    "        \n",
    "        return G_beta_E\n",
    "\n",
    "# Define the neural network A_psi(t)\n",
    "class A_psi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A_psi, self).__init__()\n",
    "        \n",
    "        # Define a fully connected neural network (modeling internal variable evolution)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 64),        # Input layer: 1 input (time t), 64 output\n",
    "            nn.ReLU(),               # Activation function\n",
    "            nn.Linear(64, 64),       # Hidden layer: 64 input, 64 output\n",
    "            nn.ReLU(),               # Activation function\n",
    "            nn.Linear(64, 1)         # Output layer: scalar output (internal variable)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        return self.fc(t)\n",
    "\n",
    "# Define the surrogate model for alpha using the auxiliary NN A_psi\n",
    "class Alpha_Surrogate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Alpha_Surrogate, self).__init__()\n",
    "        \n",
    "        # Instantiate the neural network A_psi(t)\n",
    "        self.A_psi = A_psi()\n",
    "    \n",
    "    def forward(self, t, t_0):\n",
    "        # Forward pass through the neural network A_psi at time t and t_0\n",
    "        A_t = self.A_psi(t)\n",
    "        A_t_0 = self.A_psi(t_0)\n",
    "        \n",
    "        # Compute the surrogate internal variable alpha_psi(t)\n",
    "        alpha_psi = A_t - A_t_0\n",
    "        \n",
    "        return alpha_psi\n",
    "    \n",
    "    \n",
    "    import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming W_theta, V_Phi, and Alpha_Surrogate classes are already implemented\n",
    "\n",
    "class StressModelWithAlphaSurrogate(nn.Module):\n",
    "    def __init__(self, W_theta, V_Phi, alpha_surrogate):\n",
    "        super(StressModelWithAlphaSurrogate, self).__init__()\n",
    "        \n",
    "        # Elastic potential model W_{Theta}(E)\n",
    "        self.W_theta = W_theta\n",
    "        \n",
    "        # Viscoelastic potential model V_{Phi}(E, alpha)\n",
    "        self.V_Phi = V_Phi\n",
    "        \n",
    "        # Surrogate model for alpha\n",
    "        self.alpha_surrogate = alpha_surrogate\n",
    "    \n",
    "    def forward(self, E, t, t_0):\n",
    "        # Compute the internal variable alpha using the surrogate model\n",
    "        alpha_psi = self.alpha_surrogate(t, t_0)\n",
    "        \n",
    "        # Compute W_theta(E)\n",
    "        W_E = self.W_theta(E)\n",
    "        \n",
    "        # Compute V_Phi(E, alpha_psi)\n",
    "        V_E_alpha = self.V_Phi(E, alpha_psi)\n",
    "        \n",
    "        # Compute the derivative of W_theta with respect to E (W_theta'(E))\n",
    "        grad_W_E = torch.autograd.grad(W_E, E, create_graph=True)[0]\n",
    "        \n",
    "        # Compute the derivative of V_Phi with respect to E (V_Phi'(E, alpha))\n",
    "        grad_V_E_alpha = torch.autograd.grad(V_E_alpha, E, create_graph=True)[0]\n",
    "        \n",
    "        # Total stress is the sum of both derivatives\n",
    "        stress = grad_W_E + grad_V_E_alpha\n",
    "        \n",
    "        return stress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "w_net = WNet()\n",
    "v_net = VNet()\n",
    "g_net = GNet()\n",
    "alpha_net = SurrogateAlpha()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(w_net.parameters()) + list(v_net.parameters()) + list(g_net.parameters()) + list(alpha_net.parameters()), lr=0.001)\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "num_epochs = 100  # Define how many epochs you want to train for\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for strain_batch, stress_batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Set requires_grad=True for the strain batch to compute gradients\n",
    "        strain_batch = strain_batch.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Forward pass through WNet (Elastic Potential)\n",
    "        W_E = w_net(strain_batch)\n",
    "        \n",
    "        # Compute the gradient of W_E wrt strain_batch for residual stress calculation\n",
    "        W_sum = W_E.sum()  # Summing W to get a scalar\n",
    "        grad_W_E = torch.autograd.grad(W_sum, strain_batch, create_graph=True)[0]\n",
    "\n",
    "        # Step (b): Compute residual stress, interpreted as the viscous component\n",
    "        S_residual = stress_batch - grad_W_E  # Residual stress = total stress - elastic stress\n",
    "\n",
    "        # Surrogate model for alpha evolution\n",
    "        t = torch.linspace(0, 3, strain_batch.size(0)).unsqueeze(1)  # Ensure correct shape for time\n",
    "        alpha_t = alpha_net(t)\n",
    "\n",
    "        # Compute V(Phi) (Viscoelastic Potential)\n",
    "        V_E_alpha = v_net(strain_batch, alpha_t)\n",
    "        \n",
    "        # Compute the gradient of V_E_alpha wrt alpha_t for beta (Internal stress evolution)\n",
    "        V_sum = V_E_alpha.sum()  # Summing V_E_alpha to get a scalar\n",
    "        grad_V_E_alpha = torch.autograd.grad(V_sum, alpha_t, create_graph=True)[0]\n",
    "        beta = -grad_V_E_alpha  # Beta is the negative gradient of V wrt alpha_t\n",
    "\n",
    "        # Compute G(beta, strain) (Dissipation Potential)\n",
    "        G_beta_E = g_net(beta.detach(), strain_batch)  # Detach beta to avoid second-order gradients\n",
    "\n",
    "        # Compute the time derivative of alpha (penalty term calculation)\n",
    "        alpha_sum = alpha_t.sum()  # Summing alpha_t to get a scalar\n",
    "        alpha_dot = torch.autograd.grad(alpha_sum, t, create_graph=True)[0]\n",
    "        \n",
    "        # Compute the penalty term for the internal evolution (Eq from Part 5)\n",
    "        # The penalty term forces the time evolution of alpha to follow the dissipation potential\n",
    "        penalty_term = torch.square(alpha_dot - G_beta_E).sum()\n",
    "\n",
    "        # Loss calculation: Fitting W_E + V_E_alpha to the stress and adding the penalty term\n",
    "        loss = criterion(W_E + V_E_alpha, stress_batch) + penalty_term\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmented loss function with penalty\n",
    "def augmented_loss(predicted_stress, true_stress, alpha_surrogate, alpha_evolution, lambda_penalty=0.1):\n",
    "    mse_loss = nn.MSELoss()(predicted_stress, true_stress)\n",
    "    penalty_loss = lambda_penalty * torch.mean((alpha_surrogate - alpha_evolution) ** 2)\n",
    "    total_loss = mse_loss + penalty_loss\n",
    "    return total_loss\n",
    "\n",
    "# Instantiate the models\n",
    "W_model = PotentialW()\n",
    "V_model = PotentialV()\n",
    "G_model = PotentialG()\n",
    "alpha_surrogate = AlphaSurrogate()\n",
    "\n",
    "# Instantiate the viscoelastic model with the alpha surrogate\n",
    "viscoelastic_model = ViscoelasticModelWithAlpha(W_model, V_model, G_model, alpha_surrogate)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(viscoelastic_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    # Loop through the data loader batches\n",
    "    for batch_idx, (strain_batch, true_stress_batch) in enumerate(data_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Ensure strain_batch requires grad for computing potentials W, V, and G\n",
    "        strain_batch.requires_grad_(True)\n",
    "\n",
    "        # Initialize alpha with requires_grad=True so we can compute gradients for alpha evolution\n",
    "        alpha = torch.zeros_like(strain_batch, requires_grad=True)\n",
    "\n",
    "        # Forward pass: compute predicted stress, alpha evolution, and surrogate alpha\n",
    "        predicted_stress, alpha_evolution, alpha_surrogate_output = viscoelastic_model(strain_batch, alpha)\n",
    "\n",
    "        # Compute the augmented loss (MSE + penalty for alpha evolution)\n",
    "        loss = augmented_loss(predicted_stress, true_stress_batch, alpha_surrogate_output, alpha_evolution)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss for monitoring every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and compare with real stress\n",
    "viscoelastic_model.eval()\n",
    "\n",
    "# Forward pass to get predictions on the full dataset\n",
    "with torch.no_grad():\n",
    "    # Initialize alpha for inference without requires_grad=True\n",
    "    alpha = torch.zeros_like(strain_tensor)\n",
    "\n",
    "    # Forward pass through the model to get predictions without computing gradients\n",
    "    predicted_stress_list = []\n",
    "\n",
    "    # Iterate over each strain value in the dataset\n",
    "    for i in range(len(strain_tensor)):\n",
    "        strain = strain_tensor[i].unsqueeze(0)  # Shape (1, 1)\n",
    "        alpha_i = alpha[i].unsqueeze(0)  # Shape (1, 1)\n",
    "\n",
    "        # Use the model to compute stress using the learned potentials without needing gradients\n",
    "        W_output = W_model(strain)\n",
    "        V_output = V_model(strain, alpha_i)\n",
    "\n",
    "        # Here we directly use the outputs from W_model and V_model as stress approximation\n",
    "        total_stress = W_output + V_output\n",
    "\n",
    "        # Collect the predicted stress\n",
    "        predicted_stress_list.append(total_stress.item())\n",
    "\n",
    "    # Convert the list of predicted stresses to a numpy array for plotting\n",
    "    predicted_stress = np.array(predicted_stress_list)\n",
    "\n",
    "# Convert strain and stress to numpy arrays for easier plotting\n",
    "strain_np = strain_tensor.squeeze().numpy()  # Shape (N,)\n",
    "stress_np = stress_tensor.squeeze().numpy()  # Shape (N,)\n",
    "\n",
    "# Plot the real vs predicted stress vs time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, stress_np, label=\"Real Stress\", color='b', linewidth=2)\n",
    "plt.plot(time, predicted_stress, label=\"Predicted Stress\", color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Real vs Predicted Stress (Time Domain)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the real vs predicted stress-strain curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(strain_np, stress_np, label=\"Real Stress-Strain\", color='b', linewidth=2)\n",
    "plt.plot(strain_np, predicted_stress, label=\"Predicted Stress-Strain\", color='r', linestyle='--', linewidth=2)\n",
    "plt.xlabel(\"Strain\")\n",
    "plt.ylabel(\"Stress (MPa)\")\n",
    "plt.title(\"Real vs Predicted Stress-Strain Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
