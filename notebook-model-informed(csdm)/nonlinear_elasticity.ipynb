{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of a Nonlinear Elastic Law by Black box ANN\n",
    "\n",
    "- **Tensor Definitions**:\n",
    "  - Green-Lagrange strain tensor: $\\mathbf{E} \\in \\operatorname{Sym}_3(\\mathbb{R})$\n",
    "  - Second Piola-Kirchhoff stress tensor: $\\mathbf{S} \\in \\operatorname{Sym}_3(\\mathbb{R})$\n",
    "\n",
    "- **Constitutive Mapping**:\n",
    "  - Constitutive mapping between vectorized tensors: $\\widehat{\\mathbf{S}} = \\mathbf{G}(\\widehat{\\mathbf{E}})$\n",
    "  - $\\mathbf{G}: \\mathbb{R}^6 \\rightarrow \\mathbb{R}^6$\n",
    "  - Voigt notation for strain tensor: \n",
    "    $$\\widehat{\\mathbf{E}} = [E_{11}, E_{22}, E_{33}, 2E_{23}, 2E_{13}, 2E_{12}]^T$$\n",
    "  - Voigt notation for stress tensor: \n",
    "    $$\\widehat{\\mathbf{S}} = [S_{11}, S_{22}, S_{33}, S_{23}, S_{13}, S_{12}]^T$$\n",
    "\n",
    "- **Algorithm (Forward Pass of ANN)**:\n",
    "  - **Input**: $x^{(0)}$\n",
    "  - **Output**: $\\boldsymbol{y}$\n",
    "  - **Loop for $l = 1, \\ldots, N_l - 1$**:\n",
    "    - $$\\boldsymbol{x}^{(l)} = \\phi\\left(\\mathcal{W}^{(l)} \\boldsymbol{x}^{(l-1)} + \\mathbf{b}^{(l)}\\right)$$\n",
    "  - **End Loop**\n",
    "  - **Final Output**:\n",
    "    - $$\\boldsymbol{y} = \\mathcal{W}^{(N_l)} \\boldsymbol{x}^{(N_l-1)} + \\mathbf{b}^{(N_l)}$$\n",
    "\n",
    "- **Training Data**:\n",
    "  - Set of $N_m$ strain-stress data pairs:\n",
    "    $$\\left\\{\\left(\\widehat{\\mathbf{E}}^{(m)}, \\widehat{\\mathbf{S}}^{(m)}\\right)\\right\\}_{m=1}^{N_m}$$\n",
    "\n",
    "- **Loss Function (Squared Error)**:\n",
    "  - To determine the ANN parameters:\n",
    "    $$\\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left(\\left(\\mathcal{N}_\\beta\\left(\\widehat{\\mathbf{E}}^{(m)}\\right)\\right)_k - \\widehat{S}_k^{(m)}\\right)^2$$\n",
    "  - Where:\n",
    "    - $\\left(\\right)_k$: $k$-th entry of vector $\\left(\\right)$\n",
    "    - $d$: Dimensionality of vectorized stress tensor (generally $d=6$ for a solid)\n",
    "\n",
    "- **Scaled Loss Function**:\n",
    "  - To address stress components with varying magnitudes:\n",
    "    $$\\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left(\\frac{\\mathcal{N}_\\beta\\left(\\left(\\widehat{\\mathbf{E}}^{(m)}\\right)\\right)_k - \\widehat{S}_k^{(m)}}{\\sigma_k}\\right)^2$$\n",
    "  - Where $\\sigma_k$ denotes the component-wise standard deviation of the training stress data.\n",
    "  - the loss function is minimized using a stochastic gradient descent algorithm\n",
    "  - gradients with respect to each of the ANN’s parameters are obtained exactly through automatic  differentiation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Objective of Approximation with ANN**:\n",
    "  - Approximate $\\mathbf{G}$ with ANN as: $\\mathbf{G} \\approx \\mathcal{N}_\\theta$\n",
    "  - ANN parameters: \n",
    "    $$\\theta=\\bigcup_{l=1}^{N_l}(\\mathcal{W}^{(l)}, \\mathbf{b}^{(l)})$$\n",
    "    - $\\mathbf{W}^{(l)}$: Tensor-valued parameter\n",
    "    - $\\mathbf{b}^{(l)}$: Vector-valued parameter\n",
    "\n",
    "- **Activation Function**:\n",
    "  - Nonlinear activation function used: $\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mechanics-Based Model Constraints\n",
    "\n",
    "- **General Challenges of ANN Models for Nonlinear Elasticity**:\n",
    "  - When the size of an ANN (as defined by Equation (2)) is allowed to grow, it can fit well, in the least squares sense, to data from materials governed by complicated nonlinear elastic laws.\n",
    "  - However, as a **phenomenological model**, it can violate fundamental principles in mechanics, making it less suitable for numerical simulations. This can be due to:\n",
    "    - Imperfect training\n",
    "    - Noisy training data\n",
    "    - Overfitting\n",
    "  - Lack of interpretability of ANNs makes it difficult to evaluate the physical soundness of the model parameters after training.\n",
    "\n",
    "- **Importance of Mechanics-Based Constraints**:\n",
    "  - **Objective**: To enforce mechanics-based constraints in the construction of a data-driven constitutive law to ensure physical validity.\n",
    "  - **Advantages**:\n",
    "    - Embedding **a priori knowledge** of mechanics in a data-driven model helps to:\n",
    "      - Favor learning the structure of a constitutive relation over overfitting.\n",
    "      - Reduce the model's sensitivity to noisy data.\n",
    "      - Promote robustness to inputs outside the training domain.\n",
    "    - Mechanics-based constraints act as a form of **regularization**.\n",
    "  \n",
    "- **Mechanics-Based Constraints for ANN Constitutive Models**:\n",
    "  - This section identifies and discusses **four mechanics-based constraints** that are crucial for a regression-based constitutive law.\n",
    "  - The methods for enforcing these constraints in representing a nonlinear elastic law by a regression ANN are also proposed.\n",
    "  - The constraints and associated enforcement methods are complementary—enforcing one does not compromise the formulation or enforcement of another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dynamic Stability\n",
    "\n",
    "- **Definition of Dynamic Stability**:\n",
    "  - The definition of dynamic stability of a mechanical system varies widely in the literature.\n",
    "  - In this context, it is defined as the **ability of a system to always maintain finite kinetic energy when finite work is performed on it**.\n",
    "\n",
    "- **Theorem 1**: \n",
    "  - A body described by an elastic material law is **dynamically stable** if and only if it is **hyperelastic**, meaning:\n",
    "    $$\\mathbf{S}(\\mathbf{E}) = \\frac{\\partial W}{\\partial \\mathbf{E}}(\\mathbf{E})$$\n",
    "    - Where $W: \\operatorname{Sym}_3(\\mathbb{R}) \\rightarrow \\mathbb{R}$ represents the **strain energy density** of the body.\n",
    "\n",
    "- **Proof (Inspired by Carroll)**:\n",
    "  - The proof is based on observations originally made by R.S. Rivlin.\n",
    "  - **Assumptions**:\n",
    "    - All processes considered are **isothermal**.\n",
    "    - All **traction** and **body force densities** are **time-independent** in the reference configuration.\n",
    "  - **Notations**:\n",
    "    - Reference configuration: $\\Omega$\n",
    "    - Boundary: $\\Gamma = \\partial \\Omega$\n",
    "    - Configuration-dependent quantity at time $t$: $(\\bullet)_t$\n",
    "  - **Rate of Change of Total Kinetic Energy**:\n",
    "    $$\\frac{\\mathrm{d} K}{\\mathrm{~d} t}(t) = -\\int_{\\Omega} \\mathbf{S} : \\frac{\\mathrm{d} \\mathbf{E}}{\\mathrm{~d} t} \\, \\mathrm{d} \\Omega + \\int_{\\Gamma_t} \\mathbf{t}_t \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Gamma_t + \\int_{\\Omega_t} \\rho_t \\mathbf{b} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Omega_t$$\n",
    "    - Where:\n",
    "      - $\\mathbf{t}$: Traction\n",
    "      - $\\mathbf{b}$: Body force density\n",
    "      - $\\rho$: Material density\n",
    "      - $\\mathbf{u}$: Deformation state\n",
    "      - $\\dot{\\mathbf{u}}$: Time derivative of $\\mathbf{u}$\n",
    "      - The symbol \":\" denotes the **double contraction** between two tensors.\n",
    "\n",
    "- **Transformation to Reference Configuration**:\n",
    "  - To pull all integrals back to the reference configuration:\n",
    "    $$\\mathbf{t}_t \\, \\mathrm{d} \\Gamma_t = \\mathrm{d} \\mathbf{f}_t = \\mathbf{F}_t \\, \\mathrm{d} \\mathbf{f} = \\mathbf{F}_t \\mathbf{F}_t^{-1} \\mathbf{t} \\, \\mathrm{d} \\Gamma = \\mathbf{t} \\, \\mathrm{d} \\Gamma$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dynamic Stability (Continued)\n",
    "\n",
    "- **Transformation to Reference Configuration (Continued)**:\n",
    "  - Material density transformation:\n",
    "    $$\\rho_t \\, \\mathrm{d} \\Omega_t = \\rho_t \\, \\operatorname{det}(\\mathbf{F}_t) \\, \\mathrm{d} \\Omega = \\rho \\, \\mathrm{d} \\Omega$$\n",
    "    - Where $\\mathbf{F}$ is the **deformation gradient** and $\\mathbf{f}$ is the **force vector**.\n",
    "  - The rate of change of total kinetic energy can be rewritten as:\n",
    "    $$\\frac{\\mathrm{d} K}{\\mathrm{~d} t}(t) = -\\int_{\\Omega} \\mathbf{S} : \\frac{\\mathrm{d} \\mathbf{E}}{\\mathrm{~d} t} \\, \\mathrm{d} \\Omega + \\int_{\\Gamma} \\mathbf{t} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Gamma + \\int_{\\Omega} \\rho \\mathbf{b} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Omega$$\n",
    "\n",
    "- **Cyclic Deformation and Change in Kinetic Energy**:\n",
    "  - Consider a **cyclic deformation** from time $t_a$ to $t_b$, such that $\\mathbf{u}(t_a) = \\mathbf{u}(t_b)$ everywhere in $\\Omega$, implying $\\mathbf{E}(t_a) = \\mathbf{E}(t_b)$.\n",
    "  - The change in kinetic energy over the cycle is:\n",
    "    $$\\Delta K = \\int_{t_a}^{t_b} \\left( -\\int_{\\Omega} \\mathbf{S} : \\frac{\\mathrm{d} \\mathbf{E}}{\\mathrm{~d} t} \\, \\mathrm{d} \\Omega + \\int_{\\Gamma} \\mathbf{t} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Gamma + \\int_{\\Omega} \\rho \\mathbf{b} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Omega \\right) \\mathrm{d} t$$\n",
    "\n",
    "- **Internal Energy Consideration**:\n",
    "  - For a cyclic process, the **internal energy change** is zero since the deformation and thermal states are identical at the beginning and end:\n",
    "    $$\\Delta U_{\\text{int}} = \\int_{t_a}^{t_b} \\frac{\\mathrm{d} U_{\\text{int}}}{\\mathrm{d} t} \\, \\mathrm{d} t = \\int_{t_a}^{t_b} \\int_{\\Omega} \\mathbf{S} : \\frac{\\mathrm{d} \\mathbf{E}}{\\mathrm{~d} t} \\, \\mathrm{d} \\Omega \\, \\mathrm{d} t + \\int_{t_a}^{t_b} \\left( -\\int_{\\Gamma} \\mathbf{n} \\cdot \\mathbf{q} \\, \\mathrm{d} \\Gamma + \\int_{\\Omega} \\rho q_s \\, \\mathrm{d} \\Omega \\right) \\mathrm{d} t = 0$$\n",
    "    - Where:\n",
    "      - $\\mathbf{q}$: **Heat flux**\n",
    "      - $q_s$: **Heat supplied** per unit mass\n",
    "\n",
    "- **Second Law of Thermodynamics**:\n",
    "  - Using the **second law of thermodynamics**, it can be shown that the combination of heat flux and heat supply terms is non-positive:\n",
    "    $$\\int_{t_a}^{t_b} \\left( -\\int_{\\Gamma} \\mathbf{n} \\cdot \\mathbf{q} \\, \\mathrm{d} \\Gamma + \\int_{\\Omega} \\rho q_s \\, \\mathrm{d} \\Omega \\right) \\mathrm{d} t \\leq \\int_{t_a}^{t_b} T \\frac{\\mathrm{d} S}{\\mathrm{~d} t} = 0$$\n",
    "    $$\\therefore \\int_{t_a}^{t_b} \\int_{\\Omega} \\mathbf{S} : \\frac{\\mathrm{d} \\mathbf{E}}{\\mathrm{~d} t} \\, \\mathrm{d} \\Omega \\, \\mathrm{d} t \\geq 0$$\n",
    "    - Where $S$ is the **total entropy** in the body.\n",
    "\n",
    "- **Work Done by Body Forces and Tractions**:\n",
    "  - Since body forces and tractions are assumed to be **time-independent**, they perform **zero net work** on the body, as shown in the subsequent expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dynamic Stability (Continued)\n",
    "\n",
    "- **Work Done by Body Forces and Tractions**:\n",
    "  - The work done by **tractions** over the cycle:\n",
    "    $$\\int_{t_a}^{t_b} \\int_{\\Gamma} \\mathbf{t} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Gamma \\, \\mathrm{d} t = \\int_{\\Gamma} \\int_{t_a}^{t_b} \\mathbf{t} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} t \\, \\mathrm{d} \\Gamma = \\int_{\\Gamma} \\left( \\mathbf{t} \\cdot \\mathbf{u}(t_b) - \\mathbf{t} \\cdot \\mathbf{u}(t_a) \\right) \\, \\mathrm{d} \\Gamma = 0$$\n",
    "  - The work done by **body forces** over the cycle:\n",
    "    $$\\int_{t_a}^{t_b} \\int_{\\Omega} \\mathbf{b} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} \\Omega \\, \\mathrm{d} t = \\int_{\\Omega} \\int_{t_a}^{t_b} \\mathbf{b} \\cdot \\dot{\\mathbf{u}} \\, \\mathrm{d} t \\, \\mathrm{d} \\Omega = \\int_{\\Omega} \\left( \\mathbf{b} \\cdot \\mathbf{u}(t_b) - \\mathbf{b} \\cdot \\mathbf{u}(t_a) \\right) \\, \\mathrm{d} \\Omega = 0$$\n",
    "\n",
    "- **Change in Kinetic Energy Over a Cycle**:\n",
    "  - The change in kinetic energy over a cycle can be rewritten as:\n",
    "    $$\\Delta K = K(t_b) - K(t_a) = -\\int_{\\Omega} \\int_{t_a}^{t_b} \\mathbf{S} : \\frac{\\mathrm{d} \\mathbf{E}}{\\mathrm{~d} t} \\, \\mathrm{d} t \\, \\mathrm{d} \\Omega \\leq 0$$\n",
    "    - The **inequality** is a consequence of the earlier derived conditions.\n",
    "  - **Intuitive Explanation**:\n",
    "    - The **non-positivity** of the change in kinetic energy makes intuitive sense:\n",
    "      - If $\\Delta K$ were positive, given that the cyclic process can be repeated an arbitrary number of times, it would imply the possibility of generating an **infinite amount of kinetic energy** using zero net work, which is **non-physical**.\n",
    "      - The result $\\Delta K < 0$ is also not permissible, because reversing the direction of the deformation cycle (introducing a negative sign in the expression) would lead to $\\Delta K > 0$.\n",
    "      - Hence, the only **physical and dynamically stable** possibility is:\n",
    "        $$\\Delta K = 0$$\n",
    "      - This is satisfied for arbitrary $\\Omega$ and deformation paths if and only if the integrand of the outer integral in (12) identically vanishes.\n",
    "  \n",
    "- **Conclusion on Dynamic Stability**:\n",
    "  - This condition implies:\n",
    "    $$\\mathbf{S} : \\mathrm{d} \\mathbf{E} = \\mathrm{d} W$$\n",
    "    - Which means $\\mathbf{S}$ is a **perfect differential**.\n",
    "    - **Equivalently**:\n",
    "      $$\\mathbf{S} = \\frac{\\partial W}{\\partial \\mathbf{E}}$$\n",
    "    - This condition represents the **hyperelasticity** of the material and ensures **dynamic stability**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyperelasticity via Learning Strain Energy Density\n",
    "\n",
    "- **Challenges of Standard ANN Approaches**:\n",
    "  - Even if the training data for the surrogate model comes from a **hyperelastic material**, a straightforward regression ANN mapping strains to stresses:\n",
    "    $$\\widehat{\\mathbf{S}} = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}})$$\n",
    "    cannot be expected to **necessarily satisfy** the hyperelastic condition (Equation 3).\n",
    "  - Even with **noise-free data** and **zero loss convergence**, the **interpolation** and **extrapolation** by a standard ANN inside and outside the training domain are not guaranteed to be **conservative**.\n",
    "\n",
    "- **Proposed Approach for Guaranteeing Hyperelasticity**:\n",
    "  - To ensure hyperelasticity for **arbitrary strain inputs**, it is proposed to represent the constitutive law using an ANN that learns the **strain energy density function**:\n",
    "    $$W = \\mathcal{N}_\\theta(\\widehat{\\mathbf{E}})$$\n",
    "  - **Previous Approaches**:\n",
    "    - Some previous works have relied on using **strain energy density as part of the training data**, ${}^{21}$ which is incompatible with **physical experiments**.\n",
    "    - Other works have **approximated the hyperelasticity constraint** by weakly enforcing **symmetry of the tangent modulus**. ${}^{22}$\n",
    "\n",
    "- **Novel Training Approach**:\n",
    "  - The proposed method trains the ANN to implicitly **learn the integral of the data** instead of learning the data directly.\n",
    "  - The ANN parameters $\\theta$ are determined by minimizing:\n",
    "    $$\\theta = \\underset{\\beta \\in \\mathbb{R}^{N_\\theta}}{\\operatorname{argmin}} \\sum_{m=1}^{N_m} \\sum_{k=1}^d \\left( \\frac{\\frac{\\partial \\mathcal{N}_\\beta}{\\partial \\hat{\\mathrm{k}}_k}\\left(\\widehat{\\mathbf{E}}^{(m)}\\right) - \\widehat{S}_k^{(m)}}{\\sigma_k} \\right)^2$$\n",
    "  \n",
    "- **Training Process**:\n",
    "  - The **weights** of the ANN are trained so that the **partial derivatives** of the network with respect to the input match the training stress data.\n",
    "  - This promotes the learning of a **strain energy density function** (up to an irrelevant additive constant).\n",
    "  \n",
    "- **Obtaining Stresses**:\n",
    "  - After training, the **stresses** can be obtained by differentiating the ANN with respect to the strains:\n",
    "    $$\\widehat{\\mathbf{S}} = \\frac{\\partial \\mathcal{N}_e}{\\partial \\widehat{\\mathbf{E}}}(\\widehat{\\mathbf{E}})$$\n",
    "  \n",
    "- **Resulting Properties**:\n",
    "  - The resulting strain-stress mapping is **unconditionally hyperelastic** by construction.\n",
    "  - This holds **regardless** of the strain input or the smallest value attained by the training loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Computational Efficiency via Reverse Mode Automatic Differentiation\n",
    "\n",
    "- **Remark 2: Leveraging Reverse Mode Automatic Differentiation**:\n",
    "  - **Reverse mode automatic differentiation** can be used to **exactly differentiate** a trained ANN and achieve **computational efficiency** during the online computation of stresses.\n",
    "  - It is particularly efficient for obtaining the **Jacobian** of a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, where $m \\gg n$.\n",
    "    - In the context of this article, where the ANN learns the **strain energy density**, $n = 1$ and $m = 6$.\n",
    "  - The **gradient** of the ANN with respect to all its inputs can be obtained at roughly the **same computational cost** as for a single function evaluation.\n",
    "  - In contrast, using **finite differencing** to compute the gradient would require **at least $m + 1$ function evaluations** and would be affected by **numerical errors**.\n",
    "\n",
    "- **Eager Execution vs. Graph Execution**:\n",
    "  - There is a notable difference in the **online computational cost** between **eager execution** and **graph execution** of the ANN model.\n",
    "  - **Eager Execution**:\n",
    "    - Interprets the code and executes it in **real-time**.\n",
    "    - The evaluation of the constitutive law involves evaluating $W$, constructing the **backwards graph**, and propagating through the backwards graph.\n",
    "    - This approach introduces **unnecessary computations** and **software-related computational overhead**.\n",
    "  - **Graph Execution**:\n",
    "    - Interprets the code as a **graph**.\n",
    "    - The backwards graph is constructed and **compiled offline**.\n",
    "    - Online evaluation of the constitutive law requires only **propagation through a single graph** that directly relates the strains to the stresses.\n",
    "    - This approach is **computationally more economical**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 I Objectivity\n",
    "\n",
    "- **Concept of Objectivity**:\n",
    "  - **Objectivity** is the concept of **material frame indifference**—the position or orientation of an observer should not affect any quantity of interest. ${}^{23}$\n",
    "  - An **objective scalar** has the same value for all observers.\n",
    "  - Therefore, a **strain energy density function** must be invariant to any **orthogonal transformation** of the deformation gradient.\n",
    "\n",
    "- **Mathematical Condition for Objectivity**:\n",
    "  - Let $\\widetilde{W}$ denote the strain energy density function expressed in terms of the **deformation gradient** $\\mathbf{F}$.\n",
    "  - Objectivity requires that:\n",
    "    $$\\widetilde{W}(\\mathbf{Q F}) = \\widetilde{W}(\\mathbf{F})$$\n",
    "    - For any **orthogonal matrix** $\\mathbf{Q}$ such that $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I}$.\n",
    "  - This condition is crucial for numerical simulations using **data-driven constitutive models** to ensure that the results are independent of the definition of a reference frame.\n",
    "\n",
    "- **Polar Decomposition**:\n",
    "  - Let $\\mathbf{F} = \\mathbf{R U}$ be the **polar decomposition** of the deformation gradient:\n",
    "    - $\\mathbf{R}$: Orthogonal matrix describing a **rigid body mode**.\n",
    "    - $\\mathbf{U}$: Matrix associated with the **stretch tensor**.\n",
    "  - Choosing $\\mathbf{Q} = \\mathbf{R}^T$ leads to:\n",
    "    $$\\widetilde{W}(\\mathbf{R}^T \\mathbf{F}) = \\widetilde{W}(\\mathbf{R}^T \\mathbf{R} \\mathbf{U}) = \\widetilde{W}(\\mathbf{U})$$\n",
    "    - This implies that the strain energy density function should be expressible solely in terms of $\\mathbf{U}$ and not the individual components of $\\mathbf{F}$.\n",
    "\n",
    "- **Necessary Condition for Objectivity**:\n",
    "  - There must exist some functions $W$ and $\\mathbf{T}$ such that:\n",
    "    $$\\widetilde{W}(\\mathbf{F}) = W(\\mathbf{T}(\\mathbf{U}))$$\n",
    "  - This condition is also **sufficient** because the **stretch tensor** is invariant under arbitrary orthogonal transformations of $\\mathbf{F}$:\n",
    "    $$\\mathbf{Q F} = \\mathbf{Q} (\\mathbf{R U}) = \\overline{\\mathbf{R}} \\mathbf{U}$$\n",
    "    - Where $\\overline{\\mathbf{R}} = \\mathbf{Q R}$ is an orthogonal matrix.\n",
    "\n",
    "- **Green-Lagrange Strain**:\n",
    "  - The **Green-Lagrange strain**:\n",
    "    $$\\mathbf{E} = \\frac{1}{2}(\\mathbf{U}^2 - \\mathbf{I})$$\n",
    "    - It is expressible purely as a function of $\\mathbf{U}$.\n",
    "    - Using $\\mathbf{E}$ as input to the ANN automatically guarantees the **objectivity** of $\\widetilde{W}$ through the existence of $W$.\n",
    "  - Therefore:\n",
    "    $$\\widetilde{W}(\\mathbf{Q F}) = W\\left( \\frac{1}{2} \\left( \\mathbf{F}^T \\mathbf{Q}^T \\mathbf{Q} \\mathbf{F} - \\mathbf{I} \\right) \\right) = W\\left( \\frac{1}{2} \\left( \\mathbf{F}^T \\mathbf{F} - \\mathbf{I} \\right) \\right) = \\widetilde{W}(\\mathbf{F})$$\n",
    "\n",
    "- **Compatibility with Hyperelastic Strain-Stress Mapping**:\n",
    "  - For compatibility with a **hyperelastic strain-stress map**, the **second Piola-Kirchhoff stress** $\\mathbf{S}$ must be chosen as the corresponding **stress measure**.\n",
    "  - This is because $\\mathbf{S}$ is the **energy conjugate** of $\\mathbf{E}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Material Stability\n",
    "\n",
    "- **Concept of Material Stability**:\n",
    "  - **Material stability** ensures that **small loads** do not lead to **arbitrary deformations**.\n",
    "  - Stability is ensured by the **ellipticity** of the strain energy density function $\\widetilde{W}$ with respect to the deformation gradient $\\mathbf{F}$. ${}^{24}$\n",
    "  - A strain energy density function $\\widetilde{W}(\\mathbf{F})$ is **elliptic** if and only if:\n",
    "    $$(\\mathbf{a} \\otimes \\mathbf{b}) : \\frac{\\partial \\widetilde{W}}{\\partial \\mathbf{F}} \\mathbf{F} : (\\mathbf{a} \\otimes \\mathbf{b}) \\geq 0 \\quad \\forall \\, \\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^3$$\n",
    "\n",
    "- **Ellipticity and Convexity**:\n",
    "  - For **twice differentiable** strain energy density functions, ellipticity is equivalent to **convexity** along directions corresponding to **rank-one tensors** (known as **rank-one convexity**).\n",
    "  - Physically, this implies that only **real wave speeds** are permissible in the material.\n",
    "  - Enforcing **ellipticity** a priori is challenging; therefore, a common approach in **continuum mechanics** is to enforce a **stronger mathematical property** that implies ellipticity:\n",
    "    $$\\text{convexity} \\Rightarrow \\text{polyconvexity} \\Rightarrow \\text{ellipticity}$$\n",
    "\n",
    "- **Challenges with Convexity**:\n",
    "  - **Convexity** of $\\widetilde{W}(\\mathbf{F})$ can be enforced to achieve ellipticity, but it is often **too strong** a condition, imposing **non-physical restrictions** on material behavior. ${}^{25}$\n",
    "  - The **domain** of deformation gradients with a positive determinant is **non-convex**.\n",
    "  - Enforcing convexity in $\\mathbf{F}$ excludes the **physically reasonable growth condition**:\n",
    "    $$\\widetilde{W} \\rightarrow \\infty \\quad \\text{as } \\text{det}(\\mathbf{F}) \\rightarrow 0^+$$\n",
    "    - This condition places a **barrier on material inversion**; without it, the volume of the material could collapse to a point or line using finite work.\n",
    "  - **Non-uniqueness** of the energy minimizer is also disallowed by convexity, which prevents observing instability phenomena such as **buckling**.\n",
    "  - Additionally, it can be shown through **counterexamples** that convexity of $\\widetilde{W}(\\mathbf{F})$ **violates objectivity**. ${}^{26}$\n",
    "\n",
    "- **Polyconvexity as an Alternative**:\n",
    "  - Given these limitations, the standard approach is to enforce **polyconvexity**, which is a **weaker constraint** than convexity.\n",
    "  - The strain energy density function $\\widetilde{W}(\\mathbf{F})$ is **polyconvex** if and only if it can be expressed as a **convex function** of the **minors** of $\\mathbf{F}$:\n",
    "    $$\\exists f \\text{ convex, such that } \\widetilde{W}(\\mathbf{F}) = f(\\mathbf{F}, \\operatorname{Cof}(\\mathbf{F}), \\text{det}(\\mathbf{F})), \\quad \\forall \\mathbf{F} \\in \\mathbb{R}^{3 \\times 3}$$\n",
    "    - Where $\\operatorname{Cof}(\\mathbf{F})$ denotes the **cofactor matrix** of $\\mathbf{F}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Material Stability (Continued)\n",
    "\n",
    "- **Objectivity vs. Polyconvexity**:\n",
    "  - In **Section 3.2**, **objectivity** was identified as an essential property, particularly for the **numerical exploitation** of data-driven models.\n",
    "  - Therefore, the **objective strain measure** $\\mathbf{E}$ was chosen as input to the ANN model.\n",
    "  - However, enforcing **polyconvexity** in terms of $\\mathbf{F}$ is challenging because $\\mathbf{E}$ is not generally **convex** in $\\mathbf{F}$.\n",
    "\n",
    "- **Convexity in Strain Measure $\\mathbf{E}$**:\n",
    "  - Instead, consider the **convexity** of the strain energy density function $W$ in $\\mathbf{E}$.\n",
    "  - This is equivalent to the **symmetric positive semi-definiteness** of the gradient of $\\mathbf{S}$ with respect to $\\mathbf{E}$:\n",
    "    $$\\mathbb{C} = \\frac{\\partial \\mathbf{S}}{\\partial \\mathbf{E}} \\geqslant 0$$\n",
    "\n",
    "- **Symmetric Positive Semi-Definiteness for Fourth-Order Tensors**:\n",
    "  - The concept of **symmetric positive semi-definiteness** extends to **fourth-order tensors**.\n",
    "  - A fourth-order tensor $\\mathbb{T}$ is said to be **symmetric positive semi-definite** if, for all $\\mathbf{V}, \\mathbf{W} \\in \\operatorname{Sym}_n(\\mathbb{R})$, the following conditions hold: ${}^{27}$\n",
    "    $$\\mathbf{V} : \\mathbb{T} : \\mathbf{W} = \\mathbf{W} : \\mathbb{T} : \\mathbf{V}$$\n",
    "    $$\\mathbf{V} : \\mathbb{T} : \\mathbf{V} \\geq 0$$\n",
    "\n",
    "- **Hessian of $W(\\mathbf{E}(\\mathbf{F}))$**:\n",
    "  - Double contracting the **Hessian** of $W(\\mathbf{E}(\\mathbf{F}))$ with respect to $\\mathbf{F}$ on both left and right sides by a test matrix $\\boldsymbol{\\Psi} \\in \\mathbb{R}^{3 \\times 3}$ gives:\n",
    "    $$\\boldsymbol{\\Psi} : \\frac{\\partial^2 W}{\\partial \\mathbf{F} \\partial \\mathbf{F}} \\mathbf{F} : \\boldsymbol{\\Psi} = \\mathbf{S} : (\\boldsymbol{\\Psi}^T \\boldsymbol{\\Psi}) + (\\boldsymbol{\\Psi}^T \\mathbf{F} + \\mathbf{F}^T \\boldsymbol{\\Psi}) : \\mathbb{C} : (\\boldsymbol{\\Psi}^T \\mathbf{F} + \\mathbf{F}^T \\boldsymbol{\\Psi})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftplusSquared(nn.Module):\n",
    "    def __init__(self, beta=3):\n",
    "        super(SoftplusSquared, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (1 / (2 * self.beta ** 4)) * (torch.log(1 + torch.exp(self.beta ** 2 * x))) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elasticenergy_potential(nn.Module):\n",
    "    def __init__(self, seed=42):\n",
    "        super(Elasticenergy_potential, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.architecture = [1, 64, 64, 64, 1]\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.act_fun = SoftplusSquared()\n",
    "        # Construct layers based on architecture\n",
    "        for layer_idx in range(len(self.architecture) - 1):\n",
    "            self.layers[str(layer_idx)] = nn.Linear(self.architecture[layer_idx], self.architecture[layer_idx + 1])\n",
    "        self.model = self._create_nn()\n",
    "        self.input_min, self.input_max = 0.0, 0.0\n",
    "        self.output_min, self.output_max = 0.0, 0.0\n",
    "\n",
    "\n",
    "    def _create_nn(self):\n",
    "        model = nn.Sequential()\n",
    "        for i in range(len(self.architecture) - 1):\n",
    "            model.add_module(f\"linear_{i}\", self.layers[str(i)])\n",
    "            if i < len(self.architecture) - 2:\n",
    "                model.add_module(f\"activation_{i}\", self.act_fun)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def forward(self, x, do_unscaling=True):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x_frd = (x - self.input_min) / (self.input_max - self.input_min)\n",
    "            out = x_frd.reshape(len(x), 1)\n",
    "            out = self.model(out)\n",
    "            if do_unscaling:\n",
    "                out = out * (self.output_max - self.output_min) + self.output_min\n",
    "            return out\n",
    "        else:\n",
    "            print(\"Input is not a PyTorch tensor. Converting input to suitable format...\")\n",
    "\n",
    "    def scale_dataset(self, x, y, do_shuffle=True):\n",
    "        input_size = x.size\n",
    "        self.input_min = np.min(x)\n",
    "        self.input_max = np.max(x)\n",
    "        self.output_min = np.min(y)\n",
    "        self.output_max = np.max(y)\n",
    "        x_res = (x - min(x)) / (max(x) - min(x))\n",
    "        if max(y) - min(y) != 0:\n",
    "            y_res = (y - min(y)) / (max(y) - min(y))\n",
    "        else:\n",
    "            y_res = np.zeros(len(y))\n",
    "        if do_shuffle:\n",
    "            x_res, y_res = shuffle(x_res, y_res, random_state=42)\n",
    "\n",
    "        return torch.Tensor(x_res).reshape(input_size, 1), torch.Tensor(y_res).reshape(input_size, 1)\n",
    "\n",
    "\n",
    "    def convex_training(self, input_data, target_data, epochs=25000, epsilon=30, learning_rate=0.01, do_convex_training=True):\n",
    "   \n",
    "        \"\"\"\n",
    "        Trains the model using convex optimization.\n",
    "        :param do_convex_training: apply convexity and monotonicity constraint. Default value is True\n",
    "        :param input_data: Array, the input dataset.\n",
    "        :param target_data: Array, the target dataset.\n",
    "        :param epochs: Integer, number of training epochs.\n",
    "        :param epsilon: Float, parameter for convexity constraint.\n",
    "        :param learning_rate: Float, learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        input_scaled, target_scaled = self.scale_dataset(input_data, target_data)\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        n_samples = len(input_scaled)\n",
    "        train_indices = list(range(n_samples // 2))\n",
    "        val_indices = [i for i in range(n_samples) if i not in train_indices]\n",
    "\n",
    "        x_train, y_train = input_scaled[train_indices], target_scaled[train_indices]\n",
    "        x_val, y_val = input_scaled[val_indices], target_scaled[val_indices]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=1500)\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            train_output = self.model(x_train)\n",
    "            val_output = self.model(x_val)\n",
    "\n",
    "            # Compute loss\n",
    "            train_loss = loss_func(train_output, y_train)\n",
    "            val_loss = loss_func(val_output, y_val)\n",
    "\n",
    "            # Log training process\n",
    "            if (epoch + 1) % 500 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}] || \"\n",
    "                      f\"Train Loss: {train_loss.item():.9f} || \"\n",
    "                      f\"Val Loss: {val_loss.item():.9f} || \"\n",
    "                      f\"LR: {optimizer.param_groups[0]['lr']:.0e}\")\n",
    "\n",
    "            # Learning rate scheduler step\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if do_convex_training:\n",
    "                # Apply convexity constraint\n",
    "                self._apply_convexity_constraint(epsilon)\n",
    "\n",
    "                # Apply monotonic decreasing constraint\n",
    "                # self._apply_monotonic_decreasing_constraint()\n",
    "\n",
    "            if optimizer.param_groups[0]['lr'] <= 9e-08:\n",
    "                print('LR lower than threshold')\n",
    "                break\n",
    "\n",
    "    def _apply_monotonic_decreasing_constraint(self):\n",
    "        \"\"\"\n",
    "        Applies a monotonic decreasing constraint to the first layer weights.\n",
    "        \"\"\"\n",
    "        first_layer = self.model[0]\n",
    "        first_layer.weight.data[first_layer.weight.data > 0] = 0\n",
    "\n",
    "    def _apply_convexity_constraint(self, epsilon):\n",
    "        \"\"\"\n",
    "        Applies a convexity constraint to the model parameters.\n",
    "        :param epsilon: Float, parameter for convexity constraint.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"weight\" in name and \"0.weight\" not in name:\n",
    "                param.data[param < 0] = torch.exp(param[param < 0] - epsilon)\n",
    "\n",
    "    def __str__(self):\n",
    "        print(\"#---- Neural Network architecture -------------------#\")\n",
    "        for key, layer in self.nn_architecture_dict.items():\n",
    "            print(f'{key}: {layer}')\n",
    "        print(\"#----------------------------------------------------#\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
